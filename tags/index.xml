<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tags on CS Study Group</title>
    <link>https://cse-study.github.io/tags/</link>
    <description>Recent content in Tags on CS Study Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>©2019 Notepadium.</copyright>
    
        <atom:link href="https://cse-study.github.io/tags/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Video PreTraining (VPT)</title>
      <link>https://cse-study.github.io/ai/2022-07/220702-vpt/</link>
      <pubDate>Sat, 02 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-07/220702-vpt/</guid>
      <description>&lt;p&gt;OpenAI의 &amp;ldquo;Learning to Play Minecraft with Video PreTraining (VPT)&amp;rdquo; 글을 읽고 내용을 공유합니다.&lt;/p&gt;
&lt;h3 id=&#34;video-pretraining&#34;&gt;Video PreTraining&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;OpenAI에서 Minecraft를 플레이할 수 있는 computer-using agent를 학습시켰음&lt;/li&gt;
&lt;li&gt;최종적으로는 다이아 곡괭이를 만드는 것을 학습하였는데, 이 과정에서 20분(24000 action) 정도의 사람의 게임 플레이 내용을 unlabeled video dataset 형태로 사용하였고, 플레이어를 고용해서 만든 labeled contractor data가 일부 사용되었음&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img  src=&#34;https://cdn.openai.com/vpt/overview.svg&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;center&gt;&lt;p&gt;&lt;i&gt;Taken from https://openai.com/blog/vpt/&lt;/i&gt;&lt;/p&gt;&lt;/center&gt;
&lt;ol&gt;
&lt;li&gt;인터넷 상에서 70K hour 가량의 unlabeled video 데이터 수집&lt;/li&gt;
&lt;li&gt;사람 전문가를 통해 마우스/키보드 액션이 label된 2K hour 가량의 데이터 수집. 이를 통해 비디오 프레임과 마우스/키보드 액션 사이의 관계를 뉴럴넷 모델(&lt;strong&gt;Inverse Dynamics Model, IDM&lt;/strong&gt;) 사용하여 학습.&lt;/li&gt;
&lt;li&gt;IDM 사용하여 70K unlabeled video 데이터를 라벨링한 뒤에, 해당 데이터를 사용하여 &lt;strong&gt;past frame이 주어지면 future action을 예측하는 VPT Foundation 모델&lt;/strong&gt; 학습 (Behavior Cloning, causal)
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Input이 past video frames이고, output이 action인 auto-regressive 모델!&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Foundation 모델을 획득하면 이 모델을 가지고 zero-shot으로 task를 수행할 수도 있고, fine-tuning하여 모델을 더 task-specific 해지도록 개선할 수도 있음
&lt;ul&gt;
&lt;li&gt;RL 기반 fine-tuning을 할 때는 아래의 sub-tasks를 순차적으로 잘 수행할 때 마다 reward를 제공했다고 함&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img  src=&#34;https://cdn.openai.com/vpt/diamond-pickaxe-sequence.svg&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;center&gt;&lt;p&gt;&lt;i&gt;Taken from https://openai.com/blog/vpt/&lt;/i&gt;&lt;/p&gt;&lt;/center&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://openai.com/blog/vpt/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;OpenAI, &amp;ldquo;Learning to Play Minecraft with Video PreTraining (VPT)&amp;quot;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oz5yZc9ULAc&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Yannic Kilcher YouTube, &amp;ldquo;Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos (Paper Explained)&amp;quot;&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Vision Transformer</title>
      <link>https://cse-study.github.io/ai/2022-06/220609-vision-transformer/</link>
      <pubDate>Thu, 09 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-06/220609-vision-transformer/</guid>
      <description>&lt;p&gt;Vision transformer와 DINO 논문에 대해, 관련 자료를 읽고 내용을 공유합니다.&lt;/p&gt;
&lt;h3 id=&#34;vision-transformer&#34;&gt;Vision Transformer&lt;/h3&gt;
&lt;p&gt;&lt;img  src=&#34;https://cse-study.github.io/assets/img/ViT.png&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;center&gt;&lt;p&gt;&lt;i&gt;Taken from Dosovitskiy, Alexey, et al.&lt;/i&gt;&lt;/p&gt;&lt;/center&gt;
&lt;ol&gt;
&lt;li&gt;전체 이미지를 16 x 16 size의 patch로 절단. 예를 들어 48 x 48 이미지라면 9개의 patch로 나뉨
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;ViT-Base/16&amp;rdquo; model에서 16이 의미하는 것이 patch size임&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;각각의 patch를 평탄화(16 x 16 x 3 = 768) 한 뒤에, 워드 임베딩을 만드는 것 처럼 linear projection을 통해 embedding vector의 형태로 변환&lt;/li&gt;
&lt;li&gt;해당 embedding vector에, BERT와 동일하게 CLS 토큰과 position embedding을 추가함. 이 둘은 learnable parameter 임
&lt;ul&gt;
&lt;li&gt;CLS 토큰은 patch embedding과 동일한 사이즈이며, 9개의 patch embedding들과 concat 함&lt;/li&gt;
&lt;li&gt;position embedding은 총 9 + 1 = 10개로, patch embedding에 합(+)해주는 형태로 구성&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;각 embedding vector를 하나의 워드 토큰으로 여겨, transformer의 입력으로 전달&lt;/li&gt;
&lt;li&gt;BERT와 동일하게, CLS 토큰의 최종 출력이 해당 이미지의 output class라고 가정. 따라서 transformer 출력 단에서 CLS의 embedding이 어떻게 변했는지 확인하여 inference 수행&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;dino-self-distillation-with-no-labels&#34;&gt;DINO (Self-&lt;strong&gt;di&lt;/strong&gt;stillation with &lt;strong&gt;no&lt;/strong&gt; labels)&lt;/h3&gt;
&lt;p&gt;&lt;img  src=&#34;https://cse-study.github.io/assets/img/DINO.png&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;center&gt;&lt;p&gt;&lt;i&gt;Taken from Caron, Mathilde, et al.&lt;/i&gt;&lt;/p&gt;&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;ViT를 &lt;a href=&#34;https://yuhodots.github.io/deeplearning/21-04-04/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;BYOL&lt;/a&gt;
의 manner로 학습&lt;/li&gt;
&lt;li&gt;BYOL과 달리 normalized embedding의 L2 distance를 사용하지는 않고, $p_2 \log p_1$ 형태의 cross entropy loss 사용&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Advancing the state of the art in computer vision with self-supervised Transformers and 10x more efficient training&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Dosovitskiy, Alexey, et al. &amp;ldquo;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2021.&lt;/li&gt;
&lt;li&gt;Caron, Mathilde, et al. &amp;ldquo;Emerging properties in self-supervised vision transformers.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE/CVF International Conference on Computer Vision&lt;/em&gt;. 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=h3ij3F3cPIk&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Yanick Kilcher, &amp;ldquo;DINO: Emerging Properties in Self-Supervised Vision Transformers (Facebook AI Research Explained)&amp;quot;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/dino&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/facebookresearch/dino&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Open-world Learning</title>
      <link>https://cse-study.github.io/ai/2022-06/220603-open-world-learning/</link>
      <pubDate>Fri, 03 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-06/220603-open-world-learning/</guid>
      <description>&lt;p&gt;Chen, Zhiyuan, and Bing Liu의 &amp;ldquo;Lifelong machine learning.&amp;rdquo; chapeter 5, Open-world Learning을 읽고 간단하게 정리합니다.&lt;/p&gt;
&lt;h3 id=&#34;problem-definition&#34;&gt;Problem Definition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Closed-world assumption: 모든 테스트 클래스가 이미 학습 과정에서 본 것들로 이루어짐&lt;/li&gt;
&lt;li&gt;Open-world learning: 모델 테스트 단계에서 이전에 본 적 없는 클래스가 입력되는 경우에, scratch부터 재학습 시키지 않더라도 모델이 대응할 수 있어야 함. 관련된 용어로는 cumulative learning 혹은 open-world recognition 등이 존재&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;$N$개의 class에 대해 학습한 모델이 테스트에서 해당 $N$개에 속하지 않는 unseen class를 발견한 경우, 이를 rejected $R$ set에 보관&lt;/li&gt;
&lt;li&gt;$R$ set에서 $k$개의 unseen class를 인식하고, 각 class에 알맞게 학습 데이터 (자동으로) 수집&lt;/li&gt;
&lt;li&gt;충분한 데이터가 쌓였다면 $k$개 class에 대한 incremental learning을 통해 $N+k$ class의 지식을 지닌 모델로 업데이트&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;how-to-solve-cbs-learning&#34;&gt;How to Solve?: CBS Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CBS Learning: center-based similarity space learning&lt;/li&gt;
&lt;li&gt;Classification 하려는 target class의 center feature $c$와 sample feature의, feature space 상에서의 similarity가 높도록 학습&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deep-open-classification-doc&#34;&gt;Deep Open Classification (DOC)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;1-vs.-rest layer: Softmax를 사용하는 대신 $N$개의 sigmoid layer를 last layer로 사용함. 모든 sigmoid output에 대해서 threshold $t$ (default=0.5)를 넘지 못하면 reject(unseen class로 취급)함. 그리고 reject되지 않은 경우에는 제일 높은 sigmoid output을 가진 class를 선택함.&lt;/li&gt;
&lt;li&gt;Basic DOC는 1-vs.-rest layer를 사용하며, 당시(2017) SOTA 성능을 보였음&lt;/li&gt;
&lt;li&gt;근데 생각보다 0.5 threshold를 넘어가는 unseen class가 많았음. 따라서 threshold를 통계 기반으로 정밀하게 조절할 수 있는 방법을 고안했더니 더 성능이 올랐음 (자세한 내용 책 직접 참고)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Chen, Zhiyuan, and Bing Liu. &amp;ldquo;Lifelong machine learning.&amp;rdquo; &lt;em&gt;Synthesis Lectures on Artificial Intelligence and Machine Learning&lt;/em&gt; 12.3 (2018): 1-207&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Data Augmentation for Deep Learning</title>
      <link>https://cse-study.github.io/ai/2022-05/220527-data-augmentation/</link>
      <pubDate>Fri, 27 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-05/220527-data-augmentation/</guid>
      <description>&lt;p&gt;Deep Learning 모델 성능 개선을 위한 Data Augmentation 방법들과 효과를 정리합니다.&lt;/p&gt;
&lt;h3 id=&#34;mixup-iclr-2018&#34;&gt;Mixup (ICLR 2018)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.09412.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/pdf/1710.09412.pdf&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;한 줄 요약: 두 개의 샘플에 대해서, input space와 output space를 각각 동일한 비율로 linear interpolate한 샘플 생성&lt;/li&gt;
&lt;li&gt;장점/효과: Decision boundary가 클래스에서 클래스로 선형적으로 변하기 때문에 더 smooth한 uncertainty estimation 제공&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;manifold-mixup-icml-2019&#34;&gt;Manifold Mixup (ICML 2019)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.05236&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/1806.05236&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;한 줄 요약: Itermediate layer의 representation을 mixup하는 방법&lt;/li&gt;
&lt;li&gt;장점/효과: class-specific representations을 flatten하는 효과를 가짐. 그리고 이 flat representation에 대해서 학습때 보지 못했거나 data manifold를 벗어난 샘플은 low-confidence로 예측. 즉, 헷갈리는 샘플을 어떻게든 하나의 class로 할당하기 보다는, uncertainty 높은(어느 하나로 확신하지 않는) 예측을 뱉음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;augmix-iclr-2020&#34;&gt;AugMix (ICLR 2020)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1912.02781.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/pdf/1912.02781.pdf&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;한 줄 요약: 두 개의 샘플을 합치는 방법이 아닌, 하나의 샘플에 여러 복합적인 augmentation 방법 적용한 뒤에도 동일한 예측을 하도록 KLD 형태의 consistency loss를 regularizer로 사용하는 방법&lt;/li&gt;
&lt;li&gt;장점/효과: Robustness 관점에서 좋은 성능 (Noise에 강건함)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;autoaugment-cvpr-2019&#34;&gt;AutoAugment (CVPR 2019)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1805.09501.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/pdf/1805.09501.pdf&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;한 줄 요약: 최적의 augmentation policy를 찾도록 RL 기반 학습&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Shorten, Connor, and Taghi M. Khoshgoftaar. &amp;ldquo;A survey on image data augmentation for deep learning.&amp;rdquo; Journal of big data 6.1 (2019): 1-48&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>JAX Ecosystem</title>
      <link>https://cse-study.github.io/ai/2022-05/220520-jax/</link>
      <pubDate>Fri, 20 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-05/220520-jax/</guid>
      <description>&lt;p&gt;JAX 라이브러리 관련 자료들을 읽고 정리합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;JAX: PyTorch, Tensor, MXNet보다 빠르고, NumPy 같은 array based 계산과 여러 파이썬 수학 기능들을 편리하게, 병렬 처리 가능하도록 사용할 수 있도록 구현한 라이브러리. Google Research가 개발하였음.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Differentiation&lt;/strong&gt;: Python 함수가 주어지면 grad 기능을 통해 자동으로 미분식을 알아내고,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vectorisation&lt;/strong&gt;: vamp, pmap등의 기능을 통해 병렬화가 가능하며,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JIT-compilation&lt;/strong&gt;: CPU, GPU, TPU 등, 사용 가능한 accelerators를 쉽게 조절 할 수 있도록, 그리고 빠른 컴퓨팅 속도를 위해 just-in-time compile (C++ 처럼 컴파일 후 실행하는게 아닌, 실제 실행하는 시점에 컴파일)할 수 있도록 &lt;a href=&#34;https://www.tensorflow.org/xla&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;XLA&lt;/a&gt;
를 사용&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Haiku: JAX의 함수형 패러다임을 활용하면서도 OOP 기반 neural network 모델을 편리하게 사용할 수 있도록 구현한 라이브러리&lt;/li&gt;
&lt;li&gt;이 외에도 JAX 관련 라이브러리로는 &lt;a href=&#34;&#34;&gt;Optax&lt;/a&gt;
 (gradient 변환, optimizer 관련), &lt;a href=&#34;https://github.com/deepmind/rlax&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;RLax&lt;/a&gt;
 (RL 관련), &lt;a href=&#34;https://github.com/deepmind/chex&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Chex&lt;/a&gt;
 (실험 테스팅 관련), &lt;a href=&#34;https://github.com/deepmind/jraph&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Jraph&lt;/a&gt;
 (GNN 관련) 등이 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;정리해보자면, 기존엔 cpu만 사용할 수 있었던 numpy계산을 gpu 위에서도 가능하도록 구현했고(물론 cupy가 있기는 했지만), torch나 tensorflow라는 프레임워크 위에서 한정된 실험을 하는 것이 아니라 단순 파이썬 코드 혹은 numpy 코드를 가지고도 ML 실험이 가능하도록 구현된 라이브러리라고 할 수 있음&lt;/li&gt;
&lt;li&gt;파이썬, Numpy 기본 기능들만 가지고도 ML 실험이 매우 빠르게 가능하도록 한 것이 의미가 있다고 생각하고, JAX를 사용해보지는 않았지만 최근에 Google, DeepMind 코드에서 JAX와 Haiku가 매우매우 자주 보이는 데에는 이유가 있을 것이라는 생각이 듦&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.deepmind.com/blog/using-jax-to-accelerate-our-research&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Deepmind, &amp;ldquo;Using JAX to accelerate our research&amp;rdquo;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=WdTeDXsOSj4&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;TensorFlow, &amp;ldquo;Intro to JAX: Accelerating Machine Learning research&amp;rdquo;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=0mVmRHMaOJ4&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Google Cloud Tech, &amp;ldquo;Introduction to JAX&amp;rdquo;&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>PaLM: Pathway Language Model</title>
      <link>https://cse-study.github.io/ai/2022-05/220508-palm/</link>
      <pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-05/220508-palm/</guid>
      <description>&lt;p&gt;Google AI에서 올해 4월에 발표한 Pathway Language Model(PaLM)에 대해 관련 자료를 보고 리뷰합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Google AI에서 무려 540B 파라미터를 가진 언어 모델을 학습시켰음. 모델 구조는 decoder-only Transformer로 다른 언어 생성 모델들과 큰 차이 없음&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Pathway: 구글은 2021년에 &lt;a href=&#34;https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;pathway&lt;/a&gt;
 라는 비전을 발표하였고, 이를 이루기 위해서 &lt;a href=&#34;https://arxiv.org/abs/2203.12533&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;pathway system&lt;/a&gt;
이라는 가속기 분산 계산 관리 시스템을 개발하였음. Pathway system을 통해 학습된 PaLM은 6144개의 TPU와 1500여 개의 컴퓨터를 통해 학습되었음&lt;/li&gt;
&lt;li&gt;Chain-of-thought Prompting:  &lt;a href=&#34;https://arxiv.org/abs/2201.11903.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;chain-of-thought prompting&lt;/a&gt;
 방법과 PaLM 모델을 엮어 reasoning 성능을 확연히 높이 수 있었다고 함. Chain-of-thought Prompting은 기존 데이터셋에 대해서 output에 단답이 아닌, 왜 이런 정답이 나왔는지에 대한 intermediate steps을 추가하는 방법임.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Chowdhery, Aakanksha, et al. &amp;ldquo;Palm: Scaling language modeling with pathways.&amp;rdquo; arXiv preprint arXiv:2204.02311 (2022).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=RJwPN4qNi_Y&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Google&amp;rsquo;s 540B PaLM Language Model &amp;amp; OpenAI&amp;rsquo;s DALL-E 2 Text-to-Image Revolution&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>CLIP: Connecting Text and Images</title>
      <link>https://cse-study.github.io/ai/2022-04/220428-clip/</link>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-04/220428-clip/</guid>
      <description>&lt;p&gt;OpenAI CLIP 관련 자료를 읽고 정리합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;이미지와 텍스트를 매우 잘 representation 하는 모델을 만들어, 이미지 데이터에 대한 높은 zero-shot 성능을 이끌어내고자 했음&lt;/li&gt;
&lt;li&gt;학습 방법
&lt;ol&gt;
&lt;li&gt;mini-batch N개 만큼의 이미지와 텍스트를 준비하여, N개씩 이미지 임베딩과 텍스트 임베딩을 제작&lt;/li&gt;
&lt;li&gt;그러면 이미지 임베딩과 텍스트 임베딩 대해서 N x N matrix 형태의 similarity를 계산할 수 있는데, 동일한 셋에서 나온 이미지와 텍스트의 similairty가 최대가 되도록 contrastive loss를 사용하여 모델을 학습&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Zero-shot classification 방법 (테스트)
&lt;ol&gt;
&lt;li&gt;먼저 dataset 내의 label을 텍스트 인코더에 넣어줘서 텍스트 임베딩을 미리 다 뽑아 놓은 다음에,&lt;/li&gt;
&lt;li&gt;이미지 인코더에 이미지를 입력을 제공하여, 현재 존재하는 텍스트 임베딩 중에서 어떤 것과 가장 similarity가 높은지 확인&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comment&#34;&gt;Comment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;어떠한 모든 데이터셋에 대해서도 zero-shot으로 classification 할 수 있는 강력한 모델&lt;/li&gt;
&lt;li&gt;최근에 유명한 DALL-E 도 CLIP 임베딩을 활용했다고 함&lt;/li&gt;
&lt;li&gt;Fully supervised 방식과 견줄만 하다는 것이 매우 인상적임. 좋은 representation을 배웠기 때문에 타겟 데이터셋을 한 장도 보지 않더라도 해당 데이터 셋으로만 학습시킨 모델과 견줄 수 있다는 것이니, 그 어느 알고리즘보다도 generalization 성능이 높다는 생각이 들었음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=T9XSU0pKX2E&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.youtube.com/watch?v=T9XSU0pKX2E&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openai.com/blog/clip/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;OpenAI, CLIP: Connecting Text and Images&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Radford, Alec, et al. &amp;ldquo;Learning transferable visual models from natural language supervision.&amp;rdquo; &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR, 2021&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Solving ImageNet</title>
      <link>https://cse-study.github.io/ai/2022-04/220421-solving-imagenet/</link>
      <pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-04/220421-solving-imagenet/</guid>
      <description>&lt;p&gt;이번 달에 알리바바 그룹에서 발표한 &amp;ldquo;Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results&amp;rdquo; 논문을 읽고 리뷰합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;새로운 방법을 제안하는 논문은 아니고 technical report에 가까움&lt;/li&gt;
&lt;li&gt;ImageNet dataset에 대해서, 어떤 모델 구조더라도 하이퍼파라미터 튜닝 없이 동일하게 적용할 수 있는 USI(Unified Scheme for ImageNet)을 제안. Knowledge distillation과 몇 가지 modern tricks를 사용하였고, 모든 모델에 대해서 previous SOTA를 넘었음&lt;/li&gt;
&lt;li&gt;TResNet-L 구조의 teacher model과 더불어 논문에서 제안하는 하이퍼파라미터를 사용하면, CNN, Transformer, Mobile-oriented, MLP-only 형태의 student 모델에 대해서 모두 성능이 개선된다고 함&lt;/li&gt;
&lt;li&gt;일반적인 knowledge distillation 형태(vanilla KD)와 동일하게, true label y에 대해서는 cross entropy loss를 사용하고, teacher label에 대해서는 temperature를 사용하여 soft label을 만든 뒤에 student prediction과 KLD를 계산함&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hyper-parameter tuning이 매우 time-consuming한 작업이지만 중요하기 때문에 모든 연구에서 어쩔 수 없이 수행해야했는데, 본 논문과 같은 연구 방향이 계속해서 발전하면 여러 연구자들의 시행착오 시간을 줄여줄 수 있어서 좋을 것 같다는 생각이 들었음&lt;/li&gt;
&lt;li&gt;새로운 아이디어를 제안하여 하이퍼파라미터 튜닝의 수고를 덜어주는 내용일 줄 알았는데, 그게 아니라 좋은 teacher model을 만들었더니 모든 student model에 대해서 잘했다는 technical report 형식의 논문이어서 아쉬웠음&lt;/li&gt;
&lt;li&gt;&amp;lsquo;No hyper-parameter tuning need&amp;rsquo;라고 하는데, ImageNet이 아닌 다른 데이터 셋에 적용할 때는 결국 teacher model을 하이퍼파라미터 튜닝을 통해 다시 찾아야하니 본질적인 &amp;lsquo;No hyper-parameter tuning need&amp;rsquo;는 아니라는 생각이 들었음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ridnik, Tal, et al. &amp;ldquo;Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results.&amp;rdquo; arXiv preprint arXiv:2204.03475 (2022).&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Generative Modeling by Estimating Gradients of the Data Distribution</title>
      <link>https://cse-study.github.io/ai/2022-04/220410-diffusion-model/</link>
      <pubDate>Sun, 10 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-04/220410-diffusion-model/</guid>
      <description>&lt;p&gt;Diffusion model이라는 generative model의 새로운 방향성을 제시한 Yang Song 저자의 Generative Modeling by Estimating Gradients of the Data Distribution에 대해서 리뷰합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;h4 id=&#34;backgrounds&#34;&gt;Backgrounds&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;기존 generative modeling 방법&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Likelihood-based models: autoregressive models / normalizing flow models / energy-based models (EBMs) / variational auto-encoders (VAEs) 등과 같이, PDF를 정의하고 해당 PDF의 likelihood를 maximize 하는 방법&lt;/li&gt;
&lt;li&gt;Implicit generative models: generative adversarial network과 같이, 직접적으로 PDF를 모델링하지는 않는 방법&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;기존 방법의 한계점&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Likelihood-based models: tractable normalizing constant를 보장하기 위해 모델 구조에 제약이 있고, true data distribution을 모르기 떄문에 surrogate objectives사용 (e.g. ELBO)&lt;/li&gt;
&lt;li&gt;Implicit generative models: mode collapse가 발생하기도 하는 불안정한 adversarial training에 의존&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;저자가 새로 제안하는 방법&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Score-based model: PDF를 모델링하는 것이 아닌, score function이라고 불리는 &amp;ldquo;log-PDF의 gradient&amp;rdquo; $
\nabla_\mathbf{x} \log p(\mathbf{x}) \approx \mathbf{s}_\theta(\mathbf{x})$를 모델링하는 방법&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Score-based model은 tractable normalizing constant가 필요하지 않고 성능도 매우 좋으며 likelihood 계산도 가능&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;score-based-model&#34;&gt;Score-based model&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;모델 학습 방법
&lt;ul&gt;
&lt;li&gt;모델 $\mathbf{s}(\mathbf{x})$와 데이터의 $\nabla \log p(\mathbf{x})$ 사이의 squared L2를 최소화. 즉, Fisher divergence $\mathbb{E}_{p(\mathbf{x})}[|| \nabla\log p(\mathbf{x})-\mathbf{s}(\mathbf{x})||_2^2]$ 식을 최소화&lt;/li&gt;
&lt;li&gt;하지만 우리는 $p(\mathbf x)$에 대한 특정 포인트(소유하고있는 데이터) 정보만 알고있기 때문에 위의 식을 그대로 사용할 수 없음&lt;/li&gt;
&lt;li&gt;다행히도, true $ \nabla_\mathbf{x} \log p(\mathbf{x})$를 알지 못할 때 사용할 수 있는 score-matching이라는 방법이 존재&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;데이터 샘플링 방법 (테스트)
&lt;ul&gt;
&lt;li&gt;임의의 prior $\mathbf x_0$에서 시작하여, 아래의 Langevin dynamics을 따라 $\mathbf x$를 계속해서 업데이트하면 됨 (Langevin dynamics 식 제작에 score function 필요)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img  src=&#34;https://cse-study.github.io/assets/img/lagevin.png&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Naive score-based generative modeling
&lt;ul&gt;
&lt;li&gt;전체적으로는, score-mathcing을 통해 score function을 먼저 모델링한 뒤에, 해당 score function을 가지고 정의된 Langevin dynamics을 따라 $\mathbf x$를 업데이트하여 최종 샘플링 포인트를 찾는 것 (Naive score-based generative modeling)&lt;/li&gt;
&lt;li&gt;하지만 Naive score-based generative modeling 방법은 low density regions에서는 정확하지 않다는 점 때문에, prior $\mathbf x_0$에서 시작하여 초기에 $\mathbf x$ 업데이트 과정에서 좋은 업데이트가 수행되지 않는다는 단점이 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;개선 방법
&lt;ul&gt;
&lt;li&gt;Naive score-based generative modeling의 문제점을 개선하기 위해서, 데이터에 noise perturbation을 기반으로한 &amp;lsquo;Noise Conditional Score-Based Model&amp;rsquo;와 &amp;lsquo;annealed Langevin dynamics&amp;rsquo; 방법을 적용하였음&lt;/li&gt;
&lt;li&gt;자세한 내용은 &lt;a href=&#34;https://yang-song.github.io/blog/2021/score/#score-based-generative-modeling-with-multiple-noise-perturbations&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;이 곳&lt;/a&gt;
 참고&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;해당 논문은 generative modeling 분야에 새로운 방향성을 제시한 논문으로, 후속 연구들이 계속해서 쏟아지고 있어서 기여가 큰 논문임&lt;/li&gt;
&lt;li&gt;Gradient of log PDF라는 새로운 관점을 제시한 점도 좋았지만, 개인적으로는 naive score-based generative modeling을 적용하였을 때 잘 되지 않았다는 것에서 멈추지 않고, 계속해서 개선점을 찾아내 결국엔 좋은 성능의 모델을 얻어냈다는 점이 좋았음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://yang-song.github.io/blog/2021/score/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Yang Song, Generative Modeling by Estimating Gradients of the Data Distribution&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Song, Yang, and Stefano Ermon. &amp;ldquo;Generative modeling by estimating gradients of the data distribution.&amp;rdquo; &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 32 (2019).&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>DGMR</title>
      <link>https://cse-study.github.io/ai/2022-04/20220405-dgmr/</link>
      <pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-04/20220405-dgmr/</guid>
      <description>&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;딥마인드가 영국 기상청과 협력을 통해 좋은 nowcasting(실시간 기상 예측) 성능을 보이는 DGMR (Deep Generative Model of Rain)을 개발하였습니다. 이름에 드러나 있듯 90분 후까지의 강우, 그 중에서도 사람과 경제에 큰 영향을 미치는 medium to heavy rain 예측에 중점을 둔 것으로 보입니다.&lt;/li&gt;
&lt;li&gt;기존 날씨 예측은 Ensemble numerical weather prediction(NWP) 라는 수치적 계산 방법을 이용했다고 합니다. 대기학에 대해서는 잘 모르지만 다양한 변수를 이용해 복잡한 유체 방정식을 풀며 시뮬레이션하는 방식이 아닐까 합니다(이 부분에 대해 잘 아시는 분들은 댓글 남겨주시면 감사하겠습니다). 이 방법은 단-중기 예보에서는 괜찮은 성능을 보이지만 nowcasting이라 불리는 초단기예보 정확성이 떨어지는 문제가 있습니다. 딥러닝을 이용해 예측을 개선하려는 시도도 있었으나 드물게 일어나는 일인 medium to heavy rain에 대해 좋지 않은 성능을 보였습니다.&lt;/li&gt;
&lt;li&gt;딥마인드는 이를 해결하기 위해 deep generative model을 이용했습니다. 레이더 자료를 입력하면 다음에 이어질 것으로 예상되는 이미지를 만들어 내고 이를 실제 관측된 값과 비교하며 학습하는 방식입니다. 다음은 모델에 대한 간략한 설명입니다.
&lt;ul&gt;
&lt;li&gt;5분 간격으로 촬영한 레이더 관측값 4개(즉 20분간 관찰한 정보)가 generator에 context로 들어갑니다. 학습은 두 loss function과 하나의 regularization을 통해 이루어집니다.&lt;/li&gt;
&lt;li&gt;제너레이터에 context가 들어가면 이미지가 생성됩니다. 이 중 8개 프레임이 랜덤으로 선택되어 spatial discriminator에 들어가고 실제 데이터와 비교해 loss 값을 얻습니다. Spatial discriminator은 CNN 구조를 가지는 모델입니다. 관측된 레이더 필드와 생성된 필드를 구별해서 spatial consistency를 확보하고 blurry prediction을 방지하는데 초점을 맞췄다고 합니다(첫 번째 loss).&lt;/li&gt;
&lt;li&gt;Temporal discriminator에는 생성된 이미지가 랜덤으로 크롭되어 입력됩니다. 이는 관측된 레이더 시퀀스와 생성된 레이더 시퀀스를 구별해서 temporal consistency를 확보하고 jumpy prediction을 penalize하는 목적을 가집니다(두 번째 loss).&lt;/li&gt;
&lt;li&gt;정확도를 향상시키기 위해  관측값들의 grid cell regularization과 모델이 생성한 평균값의 grid cell regularization간 차이에 패널티를 주는 항을 도입했습니다. 이 항은 모델이 위치에 따른 정확한 예측을 하고 성능 향상에 있어 중요하다고 합니다(하나의 regularization).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;모델 구조와 objective function에 대한 더 자세한 설명은 Methods에서 찾을 수 있으므로 관심이 있다면 읽어보시길 바랍니다!&lt;/li&gt;
&lt;li&gt;DGMR은 기존 시스템과 비교한 성능 평가가 이루어졌습니다. 전문인단중 89%가 DGMR이 더 우수하다고 판단했습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;도메인에 대한 지식이 없다 보니 기존의 문제점이 잘 와닿지는 않았고 문제에 대한 직관적인 이해도 많이 부족하게 느껴졌습니다. 이전에 다뤘던 화학 관련 내용이랑은 느낌이 또 다르네요.&lt;/li&gt;
&lt;li&gt;해당 모델을 개발한 연구원 수만 라부리 또한 영국 기상청의 조언 덕에 문제 해결에 대한 아이디어를 얻을 수 있었던 것 같습니다. 조언이 없었다면 좋지 않은 모델을 개발했을지도 모른다고 언급했다고 합니다(&lt;a href=&#34;https://www.technologyreview.com/2021/09/29/1036331/deepminds-ai-predicts-almost-exactly-when-and-where-its-going-to-rain/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;출처&lt;/a&gt;
).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DGMR paper: &lt;a href=&#34;https://www.nature.com/articles/s41586-021-03854-z&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Skilful precipitation nowcasting using deep generative models of radar&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;MIT Technology Review: &lt;a href=&#34;https://www.technologyreview.com/2021/09/29/1036331/deepminds-ai-predicts-almost-exactly-when-and-where-its-going-to-rain/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;DeepMind’s AI predicts almost exactly when and where it’s going to rain&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>