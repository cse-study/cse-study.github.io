<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Categories on CS Study Group</title>
    <link>https://cse-study.github.io/categories/</link>
    <description>Recent content in Categories on CS Study Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>©2019 Notepadium.</copyright>
    
        <atom:link href="https://cse-study.github.io/categories/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>DARTS: Differentiable Architecture Search</title>
      <link>https://cse-study.github.io/ai/2022-08/220827-darts/</link>
      <pubDate>Tue, 23 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-08/220827-darts/</guid>
      <description>&lt;p&gt;ICLR 2019에서 발표된 &amp;ldquo;DARTS: Differentiable Architecture Search&amp;rdquo; 논문을 읽고 내용을 공유합니다.&lt;/p&gt;
&lt;h3 id=&#34;darts-differentiable-architecture-search&#34;&gt;DARTS: Differentiable Architecture Search&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Neural Architecture Search (NAS) 연구에 대해, 기존에는 search space가 미분 불가능하다는 문제점 때문에 RL 기반으로만 연구가 진행었는데,&lt;/li&gt;
&lt;li&gt;DARTS는 search space를 미분 가능하게 정의하고 여기에 MAML의 최적화 방식과 동일한 bilevel optimization을 도입하여 gradient descent 기반의 NAS를 가능하도록 만들었음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Differentiable archtecture search: 논문에서 정의한 bilevel optimization 식을 통한 가중치 $\alpha$ 최적화 수행&lt;/li&gt;
&lt;li&gt;Discretization step: $\alpha$와 $k$ 기반으로 필요 없는 operation edge 제거&lt;/li&gt;
&lt;li&gt;Retraining for the top-$k$ strongest operations: 남은 operation edge에 대해 처음부터 다시 학습 수행&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;methodology&#34;&gt;Methodology&lt;/h3&gt;
&lt;p&gt;&lt;img  src=&#34;https://cse-study.github.io/assets/img/darts.png&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(a)의 형태면 candidate operations가 discrete한 set 형태로 정의되기 때문에 미분이 불가능한데, (b)의 형태로 수정 후에 각 operation edege에 $\alpha$라는 가중치를 달고 다음 node에 전달되기 전에 softmax function을 거치도록 형태를 변형하면 모든 candidate operations가 continuous한 형태로 연결되어 있기 때문에 미분이 가능해짐&lt;/li&gt;
&lt;li&gt;여기서 우리의 목표는 $\alpha$를 최적화하는 것이며, objective는 아래와 같이 설정이 가능함&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img  src=&#34;https://cse-study.github.io/assets/img/darts_obj.png&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\alpha$를 validation loss 기반으로 최적화하기 위해서는 $w$가 결정되어있어야 하며, $w$를 train loss 기반으로 최적화하기 위해서는 $\alpha$가 결정되어있어야 함. 본 논문은 이러한 nested formulation을 bilevel optimization을 통해서 최적화 함&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img  src=&#34;https://cse-study.github.io/assets/img/darts_algo.png&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bilevel optimization 뿐만 아니라, traning set + validation set을 합친 데이터 셋을 가지고 $\alpha, w$에 대한 coordinate descent와 SGD도 각각 수행하였는데, 성능은 bilevel optimization이 제일 좋았다고 함. 이에 대해 저자들은, 아마 $\alpha$를 training set이 포함된 데이터를 가지고 직접 최적화하였기 때문에 overfitting이 발생했을 것이라고 가정하였음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=S1eYHoC5FX&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Video PreTraining (VPT)</title>
      <link>https://cse-study.github.io/ai/2022-07/220702-vpt/</link>
      <pubDate>Sat, 02 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-07/220702-vpt/</guid>
      <description>&lt;p&gt;OpenAI의 &amp;ldquo;Learning to Play Minecraft with Video PreTraining (VPT)&amp;rdquo; 글을 읽고 내용을 공유합니다.&lt;/p&gt;
&lt;h3 id=&#34;video-pretraining&#34;&gt;Video PreTraining&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;OpenAI에서 Minecraft를 플레이할 수 있는 computer-using agent를 학습시켰음&lt;/li&gt;
&lt;li&gt;최종적으로는 다이아 곡괭이를 만드는 것을 학습하였는데, 이 과정에서 20분(24000 action) 정도의 사람의 게임 플레이 내용을 unlabeled video dataset 형태로 사용하였고, 플레이어를 고용해서 만든 labeled contractor data가 일부 사용되었음&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img  src=&#34;https://cdn.openai.com/vpt/overview.svg&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;center&gt;&lt;p&gt;&lt;i&gt;Taken from https://openai.com/blog/vpt/&lt;/i&gt;&lt;/p&gt;&lt;/center&gt;
&lt;ol&gt;
&lt;li&gt;인터넷 상에서 70K hour 가량의 unlabeled video 데이터 수집&lt;/li&gt;
&lt;li&gt;사람 전문가를 통해 마우스/키보드 액션이 label된 2K hour 가량의 데이터 수집. 이를 통해 비디오 프레임과 마우스/키보드 액션 사이의 관계를 뉴럴넷 모델(&lt;strong&gt;Inverse Dynamics Model, IDM&lt;/strong&gt;) 사용하여 학습.&lt;/li&gt;
&lt;li&gt;IDM 사용하여 70K unlabeled video 데이터를 라벨링한 뒤에, 해당 데이터를 사용하여 &lt;strong&gt;past frame이 주어지면 future action을 예측하는 VPT Foundation 모델&lt;/strong&gt; 학습 (Behavior Cloning, causal)
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Input이 past video frames이고, output이 action인 auto-regressive 모델!&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Foundation 모델을 획득하면 이 모델을 가지고 zero-shot으로 task를 수행할 수도 있고, fine-tuning하여 모델을 더 task-specific 해지도록 개선할 수도 있음
&lt;ul&gt;
&lt;li&gt;RL 기반 fine-tuning을 할 때는 아래의 sub-tasks를 순차적으로 잘 수행할 때 마다 reward를 제공했다고 함&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img  src=&#34;https://cdn.openai.com/vpt/diamond-pickaxe-sequence.svg&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;center&gt;&lt;p&gt;&lt;i&gt;Taken from https://openai.com/blog/vpt/&lt;/i&gt;&lt;/p&gt;&lt;/center&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://openai.com/blog/vpt/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;OpenAI, &amp;ldquo;Learning to Play Minecraft with Video PreTraining (VPT)&amp;quot;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oz5yZc9ULAc&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Yannic Kilcher YouTube, &amp;ldquo;Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos (Paper Explained)&amp;quot;&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Vision Transformer</title>
      <link>https://cse-study.github.io/ai/2022-06/220609-vision-transformer/</link>
      <pubDate>Thu, 09 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-06/220609-vision-transformer/</guid>
      <description>&lt;p&gt;Vision transformer와 DINO 논문에 대해, 관련 자료를 읽고 내용을 공유합니다.&lt;/p&gt;
&lt;h3 id=&#34;vision-transformer&#34;&gt;Vision Transformer&lt;/h3&gt;
&lt;p&gt;&lt;img  src=&#34;https://cse-study.github.io/assets/img/ViT.png&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;center&gt;&lt;p&gt;&lt;i&gt;Taken from Dosovitskiy, Alexey, et al.&lt;/i&gt;&lt;/p&gt;&lt;/center&gt;
&lt;ol&gt;
&lt;li&gt;전체 이미지를 16 x 16 size의 patch로 절단. 예를 들어 48 x 48 이미지라면 9개의 patch로 나뉨
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;ViT-Base/16&amp;rdquo; model에서 16이 의미하는 것이 patch size임&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;각각의 patch를 평탄화(16 x 16 x 3 = 768) 한 뒤에, 워드 임베딩을 만드는 것 처럼 linear projection을 통해 embedding vector의 형태로 변환&lt;/li&gt;
&lt;li&gt;해당 embedding vector에, BERT와 동일하게 CLS 토큰과 position embedding을 추가함. 이 둘은 learnable parameter 임
&lt;ul&gt;
&lt;li&gt;CLS 토큰은 patch embedding과 동일한 사이즈이며, 9개의 patch embedding들과 concat 함&lt;/li&gt;
&lt;li&gt;position embedding은 총 9 + 1 = 10개로, patch embedding에 합(+)해주는 형태로 구성&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;각 embedding vector를 하나의 워드 토큰으로 여겨, transformer의 입력으로 전달&lt;/li&gt;
&lt;li&gt;BERT와 동일하게, CLS 토큰의 최종 출력이 해당 이미지의 output class라고 가정. 따라서 transformer 출력 단에서 CLS의 embedding이 어떻게 변했는지 확인하여 inference 수행&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;dino-self-distillation-with-no-labels&#34;&gt;DINO (Self-&lt;strong&gt;di&lt;/strong&gt;stillation with &lt;strong&gt;no&lt;/strong&gt; labels)&lt;/h3&gt;
&lt;p&gt;&lt;img  src=&#34;https://cse-study.github.io/assets/img/DINO.png&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;center&gt;&lt;p&gt;&lt;i&gt;Taken from Caron, Mathilde, et al.&lt;/i&gt;&lt;/p&gt;&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;ViT를 &lt;a href=&#34;https://yuhodots.github.io/deeplearning/21-04-04/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;BYOL&lt;/a&gt;
의 manner로 학습&lt;/li&gt;
&lt;li&gt;BYOL과 달리 normalized embedding의 L2 distance를 사용하지는 않고, $p_2 \log p_1$ 형태의 cross entropy loss 사용&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Advancing the state of the art in computer vision with self-supervised Transformers and 10x more efficient training&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Dosovitskiy, Alexey, et al. &amp;ldquo;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. 2021.&lt;/li&gt;
&lt;li&gt;Caron, Mathilde, et al. &amp;ldquo;Emerging properties in self-supervised vision transformers.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE/CVF International Conference on Computer Vision&lt;/em&gt;. 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=h3ij3F3cPIk&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Yanick Kilcher, &amp;ldquo;DINO: Emerging Properties in Self-Supervised Vision Transformers (Facebook AI Research Explained)&amp;quot;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/dino&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/facebookresearch/dino&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Open-world Learning</title>
      <link>https://cse-study.github.io/ai/2022-06/220603-open-world-learning/</link>
      <pubDate>Fri, 03 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-06/220603-open-world-learning/</guid>
      <description>&lt;p&gt;Chen, Zhiyuan, and Bing Liu의 &amp;ldquo;Lifelong machine learning.&amp;rdquo; chapeter 5, Open-world Learning을 읽고 간단하게 정리합니다.&lt;/p&gt;
&lt;h3 id=&#34;problem-definition&#34;&gt;Problem Definition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Closed-world assumption: 모든 테스트 클래스가 이미 학습 과정에서 본 것들로 이루어짐&lt;/li&gt;
&lt;li&gt;Open-world learning: 모델 테스트 단계에서 이전에 본 적 없는 클래스가 입력되는 경우에, scratch부터 재학습 시키지 않더라도 모델이 대응할 수 있어야 함. 관련된 용어로는 cumulative learning 혹은 open-world recognition 등이 존재&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;$N$개의 class에 대해 학습한 모델이 테스트에서 해당 $N$개에 속하지 않는 unseen class를 발견한 경우, 이를 rejected $R$ set에 보관&lt;/li&gt;
&lt;li&gt;$R$ set에서 $k$개의 unseen class를 인식하고, 각 class에 알맞게 학습 데이터 (자동으로) 수집&lt;/li&gt;
&lt;li&gt;충분한 데이터가 쌓였다면 $k$개 class에 대한 incremental learning을 통해 $N+k$ class의 지식을 지닌 모델로 업데이트&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;how-to-solve-cbs-learning&#34;&gt;How to Solve?: CBS Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CBS Learning: center-based similarity space learning&lt;/li&gt;
&lt;li&gt;Classification 하려는 target class의 center feature $c$와 sample feature의, feature space 상에서의 similarity가 높도록 학습&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deep-open-classification-doc&#34;&gt;Deep Open Classification (DOC)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;1-vs.-rest layer: Softmax를 사용하는 대신 $N$개의 sigmoid layer를 last layer로 사용함. 모든 sigmoid output에 대해서 threshold $t$ (default=0.5)를 넘지 못하면 reject(unseen class로 취급)함. 그리고 reject되지 않은 경우에는 제일 높은 sigmoid output을 가진 class를 선택함.&lt;/li&gt;
&lt;li&gt;Basic DOC는 1-vs.-rest layer를 사용하며, 당시(2017) SOTA 성능을 보였음&lt;/li&gt;
&lt;li&gt;근데 생각보다 0.5 threshold를 넘어가는 unseen class가 많았음. 따라서 threshold를 통계 기반으로 정밀하게 조절할 수 있는 방법을 고안했더니 더 성능이 올랐음 (자세한 내용 책 직접 참고)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Chen, Zhiyuan, and Bing Liu. &amp;ldquo;Lifelong machine learning.&amp;rdquo; &lt;em&gt;Synthesis Lectures on Artificial Intelligence and Machine Learning&lt;/em&gt; 12.3 (2018): 1-207&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Data Augmentation for Deep Learning</title>
      <link>https://cse-study.github.io/ai/2022-05/220527-data-augmentation/</link>
      <pubDate>Fri, 27 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-05/220527-data-augmentation/</guid>
      <description>&lt;p&gt;Deep Learning 모델 성능 개선을 위한 Data Augmentation 방법들과 효과를 정리합니다.&lt;/p&gt;
&lt;h3 id=&#34;mixup-iclr-2018&#34;&gt;Mixup (ICLR 2018)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.09412.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/pdf/1710.09412.pdf&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;한 줄 요약: 두 개의 샘플에 대해서, input space와 output space를 각각 동일한 비율로 linear interpolate한 샘플 생성&lt;/li&gt;
&lt;li&gt;장점/효과: Decision boundary가 클래스에서 클래스로 선형적으로 변하기 때문에 더 smooth한 uncertainty estimation 제공&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;manifold-mixup-icml-2019&#34;&gt;Manifold Mixup (ICML 2019)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.05236&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/1806.05236&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;한 줄 요약: Itermediate layer의 representation을 mixup하는 방법&lt;/li&gt;
&lt;li&gt;장점/효과: class-specific representations을 flatten하는 효과를 가짐. 그리고 이 flat representation에 대해서 학습때 보지 못했거나 data manifold를 벗어난 샘플은 low-confidence로 예측. 즉, 헷갈리는 샘플을 어떻게든 하나의 class로 할당하기 보다는, uncertainty 높은(어느 하나로 확신하지 않는) 예측을 뱉음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;augmix-iclr-2020&#34;&gt;AugMix (ICLR 2020)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1912.02781.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/pdf/1912.02781.pdf&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;한 줄 요약: 두 개의 샘플을 합치는 방법이 아닌, 하나의 샘플에 여러 복합적인 augmentation 방법 적용한 뒤에도 동일한 예측을 하도록 KLD 형태의 consistency loss를 regularizer로 사용하는 방법&lt;/li&gt;
&lt;li&gt;장점/효과: Robustness 관점에서 좋은 성능 (Noise에 강건함)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;autoaugment-cvpr-2019&#34;&gt;AutoAugment (CVPR 2019)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1805.09501.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/pdf/1805.09501.pdf&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;한 줄 요약: 최적의 augmentation policy를 찾도록 RL 기반 학습&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Shorten, Connor, and Taghi M. Khoshgoftaar. &amp;ldquo;A survey on image data augmentation for deep learning.&amp;rdquo; Journal of big data 6.1 (2019): 1-48&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>JAX Ecosystem</title>
      <link>https://cse-study.github.io/ai/2022-05/220520-jax/</link>
      <pubDate>Fri, 20 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-05/220520-jax/</guid>
      <description>&lt;p&gt;JAX 라이브러리 관련 자료들을 읽고 정리합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;JAX: PyTorch, Tensor, MXNet보다 빠르고, NumPy 같은 array based 계산과 여러 파이썬 수학 기능들을 편리하게, 병렬 처리 가능하도록 사용할 수 있도록 구현한 라이브러리. Google Research가 개발하였음.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Differentiation&lt;/strong&gt;: Python 함수가 주어지면 grad 기능을 통해 자동으로 미분식을 알아내고,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vectorisation&lt;/strong&gt;: vamp, pmap등의 기능을 통해 병렬화가 가능하며,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JIT-compilation&lt;/strong&gt;: CPU, GPU, TPU 등, 사용 가능한 accelerators를 쉽게 조절 할 수 있도록, 그리고 빠른 컴퓨팅 속도를 위해 just-in-time compile (C++ 처럼 컴파일 후 실행하는게 아닌, 실제 실행하는 시점에 컴파일)할 수 있도록 &lt;a href=&#34;https://www.tensorflow.org/xla&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;XLA&lt;/a&gt;
를 사용&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Haiku: JAX의 함수형 패러다임을 활용하면서도 OOP 기반 neural network 모델을 편리하게 사용할 수 있도록 구현한 라이브러리&lt;/li&gt;
&lt;li&gt;이 외에도 JAX 관련 라이브러리로는 &lt;a href=&#34;&#34;&gt;Optax&lt;/a&gt;
 (gradient 변환, optimizer 관련), &lt;a href=&#34;https://github.com/deepmind/rlax&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;RLax&lt;/a&gt;
 (RL 관련), &lt;a href=&#34;https://github.com/deepmind/chex&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Chex&lt;/a&gt;
 (실험 테스팅 관련), &lt;a href=&#34;https://github.com/deepmind/jraph&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Jraph&lt;/a&gt;
 (GNN 관련) 등이 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;정리해보자면, 기존엔 cpu만 사용할 수 있었던 numpy계산을 gpu 위에서도 가능하도록 구현했고(물론 cupy가 있기는 했지만), torch나 tensorflow라는 프레임워크 위에서 한정된 실험을 하는 것이 아니라 단순 파이썬 코드 혹은 numpy 코드를 가지고도 ML 실험이 가능하도록 구현된 라이브러리라고 할 수 있음&lt;/li&gt;
&lt;li&gt;파이썬, Numpy 기본 기능들만 가지고도 ML 실험이 매우 빠르게 가능하도록 한 것이 의미가 있다고 생각하고, JAX를 사용해보지는 않았지만 최근에 Google, DeepMind 코드에서 JAX와 Haiku가 매우매우 자주 보이는 데에는 이유가 있을 것이라는 생각이 듦&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.deepmind.com/blog/using-jax-to-accelerate-our-research&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Deepmind, &amp;ldquo;Using JAX to accelerate our research&amp;rdquo;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=WdTeDXsOSj4&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;TensorFlow, &amp;ldquo;Intro to JAX: Accelerating Machine Learning research&amp;rdquo;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=0mVmRHMaOJ4&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Google Cloud Tech, &amp;ldquo;Introduction to JAX&amp;rdquo;&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>PaLM: Pathway Language Model</title>
      <link>https://cse-study.github.io/ai/2022-05/220508-palm/</link>
      <pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-05/220508-palm/</guid>
      <description>&lt;p&gt;Google AI에서 올해 4월에 발표한 Pathway Language Model(PaLM)에 대해 관련 자료를 보고 리뷰합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Google AI에서 무려 540B 파라미터를 가진 언어 모델을 학습시켰음. 모델 구조는 decoder-only Transformer로 다른 언어 생성 모델들과 큰 차이 없음&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Pathway: 구글은 2021년에 &lt;a href=&#34;https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;pathway&lt;/a&gt;
 라는 비전을 발표하였고, 이를 이루기 위해서 &lt;a href=&#34;https://arxiv.org/abs/2203.12533&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;pathway system&lt;/a&gt;
이라는 가속기 분산 계산 관리 시스템을 개발하였음. Pathway system을 통해 학습된 PaLM은 6144개의 TPU와 1500여 개의 컴퓨터를 통해 학습되었음&lt;/li&gt;
&lt;li&gt;Chain-of-thought Prompting:  &lt;a href=&#34;https://arxiv.org/abs/2201.11903.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;chain-of-thought prompting&lt;/a&gt;
 방법과 PaLM 모델을 엮어 reasoning 성능을 확연히 높이 수 있었다고 함. Chain-of-thought Prompting은 기존 데이터셋에 대해서 output에 단답이 아닌, 왜 이런 정답이 나왔는지에 대한 intermediate steps을 추가하는 방법임.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Chowdhery, Aakanksha, et al. &amp;ldquo;Palm: Scaling language modeling with pathways.&amp;rdquo; arXiv preprint arXiv:2204.02311 (2022).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=RJwPN4qNi_Y&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Google&amp;rsquo;s 540B PaLM Language Model &amp;amp; OpenAI&amp;rsquo;s DALL-E 2 Text-to-Image Revolution&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>CLIP: Connecting Text and Images</title>
      <link>https://cse-study.github.io/ai/2022-04/220428-clip/</link>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-04/220428-clip/</guid>
      <description>&lt;p&gt;OpenAI CLIP 관련 자료를 읽고 정리합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;이미지와 텍스트를 매우 잘 representation 하는 모델을 만들어, 이미지 데이터에 대한 높은 zero-shot 성능을 이끌어내고자 했음&lt;/li&gt;
&lt;li&gt;학습 방법
&lt;ol&gt;
&lt;li&gt;mini-batch N개 만큼의 이미지와 텍스트를 준비하여, N개씩 이미지 임베딩과 텍스트 임베딩을 제작&lt;/li&gt;
&lt;li&gt;그러면 이미지 임베딩과 텍스트 임베딩 대해서 N x N matrix 형태의 similarity를 계산할 수 있는데, 동일한 셋에서 나온 이미지와 텍스트의 similairty가 최대가 되도록 contrastive loss를 사용하여 모델을 학습&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Zero-shot classification 방법 (테스트)
&lt;ol&gt;
&lt;li&gt;먼저 dataset 내의 label을 텍스트 인코더에 넣어줘서 텍스트 임베딩을 미리 다 뽑아 놓은 다음에,&lt;/li&gt;
&lt;li&gt;이미지 인코더에 이미지를 입력을 제공하여, 현재 존재하는 텍스트 임베딩 중에서 어떤 것과 가장 similarity가 높은지 확인&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comment&#34;&gt;Comment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;어떠한 모든 데이터셋에 대해서도 zero-shot으로 classification 할 수 있는 강력한 모델&lt;/li&gt;
&lt;li&gt;최근에 유명한 DALL-E 도 CLIP 임베딩을 활용했다고 함&lt;/li&gt;
&lt;li&gt;Fully supervised 방식과 견줄만 하다는 것이 매우 인상적임. 좋은 representation을 배웠기 때문에 타겟 데이터셋을 한 장도 보지 않더라도 해당 데이터 셋으로만 학습시킨 모델과 견줄 수 있다는 것이니, 그 어느 알고리즘보다도 generalization 성능이 높다는 생각이 들었음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=T9XSU0pKX2E&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.youtube.com/watch?v=T9XSU0pKX2E&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openai.com/blog/clip/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;OpenAI, CLIP: Connecting Text and Images&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Radford, Alec, et al. &amp;ldquo;Learning transferable visual models from natural language supervision.&amp;rdquo; &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR, 2021&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Solving ImageNet</title>
      <link>https://cse-study.github.io/ai/2022-04/220421-solving-imagenet/</link>
      <pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-04/220421-solving-imagenet/</guid>
      <description>&lt;p&gt;이번 달에 알리바바 그룹에서 발표한 &amp;ldquo;Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results&amp;rdquo; 논문을 읽고 리뷰합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;새로운 방법을 제안하는 논문은 아니고 technical report에 가까움&lt;/li&gt;
&lt;li&gt;ImageNet dataset에 대해서, 어떤 모델 구조더라도 하이퍼파라미터 튜닝 없이 동일하게 적용할 수 있는 USI(Unified Scheme for ImageNet)을 제안. Knowledge distillation과 몇 가지 modern tricks를 사용하였고, 모든 모델에 대해서 previous SOTA를 넘었음&lt;/li&gt;
&lt;li&gt;TResNet-L 구조의 teacher model과 더불어 논문에서 제안하는 하이퍼파라미터를 사용하면, CNN, Transformer, Mobile-oriented, MLP-only 형태의 student 모델에 대해서 모두 성능이 개선된다고 함&lt;/li&gt;
&lt;li&gt;일반적인 knowledge distillation 형태(vanilla KD)와 동일하게, true label y에 대해서는 cross entropy loss를 사용하고, teacher label에 대해서는 temperature를 사용하여 soft label을 만든 뒤에 student prediction과 KLD를 계산함&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hyper-parameter tuning이 매우 time-consuming한 작업이지만 중요하기 때문에 모든 연구에서 어쩔 수 없이 수행해야했는데, 본 논문과 같은 연구 방향이 계속해서 발전하면 여러 연구자들의 시행착오 시간을 줄여줄 수 있어서 좋을 것 같다는 생각이 들었음&lt;/li&gt;
&lt;li&gt;새로운 아이디어를 제안하여 하이퍼파라미터 튜닝의 수고를 덜어주는 내용일 줄 알았는데, 그게 아니라 좋은 teacher model을 만들었더니 모든 student model에 대해서 잘했다는 technical report 형식의 논문이어서 아쉬웠음&lt;/li&gt;
&lt;li&gt;&amp;lsquo;No hyper-parameter tuning need&amp;rsquo;라고 하는데, ImageNet이 아닌 다른 데이터 셋에 적용할 때는 결국 teacher model을 하이퍼파라미터 튜닝을 통해 다시 찾아야하니 본질적인 &amp;lsquo;No hyper-parameter tuning need&amp;rsquo;는 아니라는 생각이 들었음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ridnik, Tal, et al. &amp;ldquo;Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results.&amp;rdquo; arXiv preprint arXiv:2204.03475 (2022).&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Generative Modeling by Estimating Gradients of the Data Distribution</title>
      <link>https://cse-study.github.io/ai/2022-04/220410-diffusion-model/</link>
      <pubDate>Sun, 10 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-04/220410-diffusion-model/</guid>
      <description>&lt;p&gt;Diffusion model이라는 generative model의 새로운 방향성을 제시한 Yang Song 저자의 Generative Modeling by Estimating Gradients of the Data Distribution에 대해서 리뷰합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;h4 id=&#34;backgrounds&#34;&gt;Backgrounds&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;기존 generative modeling 방법&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Likelihood-based models: autoregressive models / normalizing flow models / energy-based models (EBMs) / variational auto-encoders (VAEs) 등과 같이, PDF를 정의하고 해당 PDF의 likelihood를 maximize 하는 방법&lt;/li&gt;
&lt;li&gt;Implicit generative models: generative adversarial network과 같이, 직접적으로 PDF를 모델링하지는 않는 방법&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;기존 방법의 한계점&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Likelihood-based models: tractable normalizing constant를 보장하기 위해 모델 구조에 제약이 있고, true data distribution을 모르기 떄문에 surrogate objectives사용 (e.g. ELBO)&lt;/li&gt;
&lt;li&gt;Implicit generative models: mode collapse가 발생하기도 하는 불안정한 adversarial training에 의존&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;저자가 새로 제안하는 방법&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Score-based model: PDF를 모델링하는 것이 아닌, score function이라고 불리는 &amp;ldquo;log-PDF의 gradient&amp;rdquo; $
\nabla_\mathbf{x} \log p(\mathbf{x}) \approx \mathbf{s}_\theta(\mathbf{x})$를 모델링하는 방법&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Score-based model은 tractable normalizing constant가 필요하지 않고 성능도 매우 좋으며 likelihood 계산도 가능&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;score-based-model&#34;&gt;Score-based model&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;모델 학습 방법
&lt;ul&gt;
&lt;li&gt;모델 $\mathbf{s}(\mathbf{x})$와 데이터의 $\nabla \log p(\mathbf{x})$ 사이의 squared L2를 최소화. 즉, Fisher divergence $\mathbb{E}_{p(\mathbf{x})}[|| \nabla\log p(\mathbf{x})-\mathbf{s}(\mathbf{x})||_2^2]$ 식을 최소화&lt;/li&gt;
&lt;li&gt;하지만 우리는 $p(\mathbf x)$에 대한 특정 포인트(소유하고있는 데이터) 정보만 알고있기 때문에 위의 식을 그대로 사용할 수 없음&lt;/li&gt;
&lt;li&gt;다행히도, true $ \nabla_\mathbf{x} \log p(\mathbf{x})$를 알지 못할 때 사용할 수 있는 score-matching이라는 방법이 존재&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;데이터 샘플링 방법 (테스트)
&lt;ul&gt;
&lt;li&gt;임의의 prior $\mathbf x_0$에서 시작하여, 아래의 Langevin dynamics을 따라 $\mathbf x$를 계속해서 업데이트하면 됨 (Langevin dynamics 식 제작에 score function 필요)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img  src=&#34;https://cse-study.github.io/assets/img/lagevin.png&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Naive score-based generative modeling
&lt;ul&gt;
&lt;li&gt;전체적으로는, score-mathcing을 통해 score function을 먼저 모델링한 뒤에, 해당 score function을 가지고 정의된 Langevin dynamics을 따라 $\mathbf x$를 업데이트하여 최종 샘플링 포인트를 찾는 것 (Naive score-based generative modeling)&lt;/li&gt;
&lt;li&gt;하지만 Naive score-based generative modeling 방법은 low density regions에서는 정확하지 않다는 점 때문에, prior $\mathbf x_0$에서 시작하여 초기에 $\mathbf x$ 업데이트 과정에서 좋은 업데이트가 수행되지 않는다는 단점이 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;개선 방법
&lt;ul&gt;
&lt;li&gt;Naive score-based generative modeling의 문제점을 개선하기 위해서, 데이터에 noise perturbation을 기반으로한 &amp;lsquo;Noise Conditional Score-Based Model&amp;rsquo;와 &amp;lsquo;annealed Langevin dynamics&amp;rsquo; 방법을 적용하였음&lt;/li&gt;
&lt;li&gt;자세한 내용은 &lt;a href=&#34;https://yang-song.github.io/blog/2021/score/#score-based-generative-modeling-with-multiple-noise-perturbations&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;이 곳&lt;/a&gt;
 참고&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;해당 논문은 generative modeling 분야에 새로운 방향성을 제시한 논문으로, 후속 연구들이 계속해서 쏟아지고 있어서 기여가 큰 논문임&lt;/li&gt;
&lt;li&gt;Gradient of log PDF라는 새로운 관점을 제시한 점도 좋았지만, 개인적으로는 naive score-based generative modeling을 적용하였을 때 잘 되지 않았다는 것에서 멈추지 않고, 계속해서 개선점을 찾아내 결국엔 좋은 성능의 모델을 얻어냈다는 점이 좋았음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://yang-song.github.io/blog/2021/score/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Yang Song, Generative Modeling by Estimating Gradients of the Data Distribution&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Song, Yang, and Stefano Ermon. &amp;ldquo;Generative modeling by estimating gradients of the data distribution.&amp;rdquo; &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 32 (2019).&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>