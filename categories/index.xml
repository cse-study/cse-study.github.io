<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Categories on CS Study Group</title>
    <link>https://cse-study.github.io/categories/</link>
    <description>Recent content in Categories on CS Study Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>©2019 Notepadium.</copyright>
    
        <atom:link href="https://cse-study.github.io/categories/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PaLM: Pathway Language Model</title>
      <link>https://cse-study.github.io/ai/2022-05/220508-palm/</link>
      <pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-05/220508-palm/</guid>
      <description>&lt;p&gt;Google AI에서 올해 4월에 발표한 Pathway Language Model(PaLM)에 대해 관련 자료를 보고 리뷰합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Google AI에서 무려 540B 파라미터를 가진 언어 모델을 학습시켰음. 모델 구조는 decoder-only Transformer로 다른 언어 생성 모델들과 큰 차이 없음&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Pathway: 구글은 2021년에 &lt;a href=&#34;https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;pathway&lt;/a&gt;
 라는 비전을 발표하였고, 이를 이루기 위해서 &lt;a href=&#34;https://arxiv.org/abs/2203.12533&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;pathway system&lt;/a&gt;
이라는 가속기 분산 계산 관리 시스템을 개발하였음. Pathway system을 통해 학습된 PaLM은 6144개의 TPU와 1500여 개의 컴퓨터를 통해 학습되었음&lt;/li&gt;
&lt;li&gt;Chain-of-thought Prompting:  &lt;a href=&#34;https://arxiv.org/abs/2201.11903.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;chain-of-thought prompting&lt;/a&gt;
 방법과 PaLM 모델을 엮어 reasoning 성능을 확연히 높이 수 있었다고 함. Chain-of-thought Prompting은 기존 데이터셋에 대해서 output에 단답이 아닌, 왜 이런 정답이 나왔는지에 대한 intermediate steps을 추가하는 방법임.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Chowdhery, Aakanksha, et al. &amp;ldquo;Palm: Scaling language modeling with pathways.&amp;rdquo; arXiv preprint arXiv:2204.02311 (2022).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=RJwPN4qNi_Y&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Google&amp;rsquo;s 540B PaLM Language Model &amp;amp; OpenAI&amp;rsquo;s DALL-E 2 Text-to-Image Revolution&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>CLIP: Connecting Text and Images</title>
      <link>https://cse-study.github.io/ai/2022-04/220428-clip/</link>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-04/220428-clip/</guid>
      <description>&lt;p&gt;OpenAI CLIP 관련 자료를 읽고 정리합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;이미지와 텍스트를 매우 잘 representation 하는 모델을 만들어, 이미지 데이터에 대한 높은 zero-shot 성능을 이끌어내고자 했음&lt;/li&gt;
&lt;li&gt;학습 방법
&lt;ol&gt;
&lt;li&gt;mini-batch N개 만큼의 이미지와 텍스트를 준비하여, N개씩 이미지 임베딩과 텍스트 임베딩을 제작&lt;/li&gt;
&lt;li&gt;그러면 이미지 임베딩과 텍스트 임베딩 대해서 N x N matrix 형태의 similarity를 계산할 수 있는데, 동일한 셋에서 나온 이미지와 텍스트의 similairty가 최대가 되도록 contrastive loss를 사용하여 모델을 학습&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Zero-shot classification 방법 (테스트)
&lt;ol&gt;
&lt;li&gt;먼저 dataset 내의 label을 텍스트 인코더에 넣어줘서 텍스트 임베딩을 미리 다 뽑아 놓은 다음에,&lt;/li&gt;
&lt;li&gt;이미지 인코더에 이미지를 입력을 제공하여, 현재 존재하는 텍스트 임베딩 중에서 어떤 것과 가장 similarity가 높은지 확인&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comment&#34;&gt;Comment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;어떠한 모든 데이터셋에 대해서도 zero-shot으로 classification 할 수 있는 강력한 모델&lt;/li&gt;
&lt;li&gt;최근에 유명한 DALL-E 도 CLIP 임베딩을 활용했다고 함&lt;/li&gt;
&lt;li&gt;Fully supervised 방식과 견줄만 하다는 것이 매우 인상적임. 좋은 representation을 배웠기 때문에 타겟 데이터셋을 한 장도 보지 않더라도 해당 데이터 셋으로만 학습시킨 모델과 견줄 수 있다는 것이니, 그 어느 알고리즘보다도 generalization 성능이 높다는 생각이 들었음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=T9XSU0pKX2E&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.youtube.com/watch?v=T9XSU0pKX2E&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openai.com/blog/clip/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;OpenAI, CLIP: Connecting Text and Images&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Radford, Alec, et al. &amp;ldquo;Learning transferable visual models from natural language supervision.&amp;rdquo; &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;. PMLR, 2021&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Solving ImageNet</title>
      <link>https://cse-study.github.io/ai/2022-04/220421-solving-imagenet/</link>
      <pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-04/220421-solving-imagenet/</guid>
      <description>&lt;p&gt;이번 달에 알리바바 그룹에서 발표한 &amp;ldquo;Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results&amp;rdquo; 논문을 읽고 리뷰합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;새로운 방법을 제안하는 논문은 아니고 technical report에 가까움&lt;/li&gt;
&lt;li&gt;ImageNet dataset에 대해서, 어떤 모델 구조더라도 하이퍼파라미터 튜닝 없이 동일하게 적용할 수 있는 USI(Unified Scheme for ImageNet)을 제안. Knowledge distillation과 몇 가지 modern tricks를 사용하였고, 모든 모델에 대해서 previous SOTA를 넘었음&lt;/li&gt;
&lt;li&gt;TResNet-L 구조의 teacher model과 더불어 논문에서 제안하는 하이퍼파라미터를 사용하면, CNN, Transformer, Mobile-oriented, MLP-only 형태의 student 모델에 대해서 모두 성능이 개선된다고 함&lt;/li&gt;
&lt;li&gt;일반적인 knowledge distillation 형태(vanilla KD)와 동일하게, true label y에 대해서는 cross entropy loss를 사용하고, teacher label에 대해서는 temperature를 사용하여 soft label을 만든 뒤에 student prediction과 KLD를 계산함&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hyper-parameter tuning이 매우 time-consuming한 작업이지만 중요하기 때문에 모든 연구에서 어쩔 수 없이 수행해야했는데, 본 논문과 같은 연구 방향이 계속해서 발전하면 여러 연구자들의 시행착오 시간을 줄여줄 수 있어서 좋을 것 같다는 생각이 들었음&lt;/li&gt;
&lt;li&gt;새로운 아이디어를 제안하여 하이퍼파라미터 튜닝의 수고를 덜어주는 내용일 줄 알았는데, 그게 아니라 좋은 teacher model을 만들었더니 모든 student model에 대해서 잘했다는 technical report 형식의 논문이어서 아쉬웠음&lt;/li&gt;
&lt;li&gt;&amp;lsquo;No hyper-parameter tuning need&amp;rsquo;라고 하는데, ImageNet이 아닌 다른 데이터 셋에 적용할 때는 결국 teacher model을 하이퍼파라미터 튜닝을 통해 다시 찾아야하니 본질적인 &amp;lsquo;No hyper-parameter tuning need&amp;rsquo;는 아니라는 생각이 들었음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ridnik, Tal, et al. &amp;ldquo;Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results.&amp;rdquo; arXiv preprint arXiv:2204.03475 (2022).&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Generative Modeling by Estimating Gradients of the Data Distribution</title>
      <link>https://cse-study.github.io/ai/2022-04/220410-diffusion-model/</link>
      <pubDate>Sun, 10 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-04/220410-diffusion-model/</guid>
      <description>&lt;p&gt;Diffusion model이라는 generative model의 새로운 방향성을 제시한 Yang Song 저자의 Generative Modeling by Estimating Gradients of the Data Distribution에 대해서 리뷰합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;h4 id=&#34;backgrounds&#34;&gt;Backgrounds&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;기존 generative modeling 방법&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Likelihood-based models: autoregressive models / normalizing flow models / energy-based models (EBMs) / variational auto-encoders (VAEs) 등과 같이, PDF를 정의하고 해당 PDF의 likelihood를 maximize 하는 방법&lt;/li&gt;
&lt;li&gt;Implicit generative models: generative adversarial network과 같이, 직접적으로 PDF를 모델링하지는 않는 방법&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;기존 방법의 한계점&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Likelihood-based models: tractable normalizing constant를 보장하기 위해 모델 구조에 제약이 있고, true data distribution을 모르기 떄문에 surrogate objectives사용 (e.g. ELBO)&lt;/li&gt;
&lt;li&gt;Implicit generative models: mode collapse가 발생하기도 하는 불안정한 adversarial training에 의존&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;저자가 새로 제안하는 방법&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Score-based model: PDF를 모델링하는 것이 아닌, score function이라고 불리는 &amp;ldquo;log-PDF의 gradient&amp;rdquo; $
\nabla_\mathbf{x} \log p(\mathbf{x}) \approx \mathbf{s}_\theta(\mathbf{x})$를 모델링하는 방법&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Score-based model은 tractable normalizing constant가 필요하지 않고 성능도 매우 좋으며 likelihood 계산도 가능&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;score-based-model&#34;&gt;Score-based model&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;모델 학습 방법
&lt;ul&gt;
&lt;li&gt;모델 $\mathbf{s}(\mathbf{x})$와 데이터의 $\nabla \log p(\mathbf{x})$ 사이의 squared L2를 최소화. 즉, Fisher divergence $\mathbb{E}_{p(\mathbf{x})}[|| \nabla\log p(\mathbf{x})-\mathbf{s}(\mathbf{x})||_2^2]$ 식을 최소화&lt;/li&gt;
&lt;li&gt;하지만 우리는 $p(\mathbf x)$에 대한 특정 포인트(소유하고있는 데이터) 정보만 알고있기 때문에 위의 식을 그대로 사용할 수 없음&lt;/li&gt;
&lt;li&gt;다행히도, true $ \nabla_\mathbf{x} \log p(\mathbf{x})$를 알지 못할 때 사용할 수 있는 score-matching이라는 방법이 존재&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;데이터 샘플링 방법 (테스트)
&lt;ul&gt;
&lt;li&gt;임의의 prior $\mathbf x_0$에서 시작하여, 아래의 Langevin dynamics을 따라 $\mathbf x$를 계속해서 업데이트하면 됨 (Langevin dynamics 식 제작에 score function 필요)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img  src=&#34;https://cse-study.github.io/assets/img/lagevin.png&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Naive score-based generative modeling
&lt;ul&gt;
&lt;li&gt;전체적으로는, score-mathcing을 통해 score function을 먼저 모델링한 뒤에, 해당 score function을 가지고 정의된 Langevin dynamics을 따라 $\mathbf x$를 업데이트하여 최종 샘플링 포인트를 찾는 것 (Naive score-based generative modeling)&lt;/li&gt;
&lt;li&gt;하지만 Naive score-based generative modeling 방법은 low density regions에서는 정확하지 않다는 점 때문에, prior $\mathbf x_0$에서 시작하여 초기에 $\mathbf x$ 업데이트 과정에서 좋은 업데이트가 수행되지 않는다는 단점이 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;개선 방법
&lt;ul&gt;
&lt;li&gt;Naive score-based generative modeling의 문제점을 개선하기 위해서, 데이터에 noise perturbation을 기반으로한 &amp;lsquo;Noise Conditional Score-Based Model&amp;rsquo;와 &amp;lsquo;annealed Langevin dynamics&amp;rsquo; 방법을 적용하였음&lt;/li&gt;
&lt;li&gt;자세한 내용은 &lt;a href=&#34;https://yang-song.github.io/blog/2021/score/#score-based-generative-modeling-with-multiple-noise-perturbations&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;이 곳&lt;/a&gt;
 참고&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;해당 논문은 generative modeling 분야에 새로운 방향성을 제시한 논문으로, 후속 연구들이 계속해서 쏟아지고 있어서 기여가 큰 논문임&lt;/li&gt;
&lt;li&gt;Gradient of log PDF라는 새로운 관점을 제시한 점도 좋았지만, 개인적으로는 naive score-based generative modeling을 적용하였을 때 잘 되지 않았다는 것에서 멈추지 않고, 계속해서 개선점을 찾아내 결국엔 좋은 성능의 모델을 얻어냈다는 점이 좋았음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://yang-song.github.io/blog/2021/score/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Yang Song, Generative Modeling by Estimating Gradients of the Data Distribution&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Song, Yang, and Stefano Ermon. &amp;ldquo;Generative modeling by estimating gradients of the data distribution.&amp;rdquo; &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 32 (2019).&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>DGMR</title>
      <link>https://cse-study.github.io/ai/2022-04/20220405-dgmr/</link>
      <pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-04/20220405-dgmr/</guid>
      <description>&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;딥마인드가 영국 기상청과 협력을 통해 좋은 nowcasting(실시간 기상 예측) 성능을 보이는 DGMR (Deep Generative Model of Rain)을 개발하였습니다. 이름에 드러나 있듯 90분 후까지의 강우, 그 중에서도 사람과 경제에 큰 영향을 미치는 medium to heavy rain 예측에 중점을 둔 것으로 보입니다.&lt;/li&gt;
&lt;li&gt;기존 날씨 예측은 Ensemble numerical weather prediction(NWP) 라는 수치적 계산 방법을 이용했다고 합니다. 대기학에 대해서는 잘 모르지만 다양한 변수를 이용해 복잡한 유체 방정식을 풀며 시뮬레이션하는 방식이 아닐까 합니다(이 부분에 대해 잘 아시는 분들은 댓글 남겨주시면 감사하겠습니다). 이 방법은 단-중기 예보에서는 괜찮은 성능을 보이지만 nowcasting이라 불리는 초단기예보 정확성이 떨어지는 문제가 있습니다. 딥러닝을 이용해 예측을 개선하려는 시도도 있었으나 드물게 일어나는 일인 medium to heavy rain에 대해 좋지 않은 성능을 보였습니다.&lt;/li&gt;
&lt;li&gt;딥마인드는 이를 해결하기 위해 deep generative model을 이용했습니다. 레이더 자료를 입력하면 다음에 이어질 것으로 예상되는 이미지를 만들어 내고 이를 실제 관측된 값과 비교하며 학습하는 방식입니다. 다음은 모델에 대한 간략한 설명입니다.
&lt;ul&gt;
&lt;li&gt;5분 간격으로 촬영한 레이더 관측값 4개(즉 20분간 관찰한 정보)가 generator에 context로 들어갑니다. 학습은 두 loss function과 하나의 regularization을 통해 이루어집니다.&lt;/li&gt;
&lt;li&gt;제너레이터에 context가 들어가면 이미지가 생성됩니다. 이 중 8개 프레임이 랜덤으로 선택되어 spatial discriminator에 들어가고 실제 데이터와 비교해 loss 값을 얻습니다. Spatial discriminator은 CNN 구조를 가지는 모델입니다. 관측된 레이더 필드와 생성된 필드를 구별해서 spatial consistency를 확보하고 blurry prediction을 방지하는데 초점을 맞췄다고 합니다(첫 번째 loss).&lt;/li&gt;
&lt;li&gt;Temporal discriminator에는 생성된 이미지가 랜덤으로 크롭되어 입력됩니다. 이는 관측된 레이더 시퀀스와 생성된 레이더 시퀀스를 구별해서 temporal consistency를 확보하고 jumpy prediction을 penalize하는 목적을 가집니다(두 번째 loss).&lt;/li&gt;
&lt;li&gt;정확도를 향상시키기 위해  관측값들의 grid cell regularization과 모델이 생성한 평균값의 grid cell regularization간 차이에 패널티를 주는 항을 도입했습니다. 이 항은 모델이 위치에 따른 정확한 예측을 하고 성능 향상에 있어 중요하다고 합니다(하나의 regularization).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;모델 구조와 objective function에 대한 더 자세한 설명은 Methods에서 찾을 수 있으므로 관심이 있다면 읽어보시길 바랍니다!&lt;/li&gt;
&lt;li&gt;DGMR은 기존 시스템과 비교한 성능 평가가 이루어졌습니다. 전문인단중 89%가 DGMR이 더 우수하다고 판단했습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;도메인에 대한 지식이 없다 보니 기존의 문제점이 잘 와닿지는 않았고 문제에 대한 직관적인 이해도 많이 부족하게 느껴졌습니다. 이전에 다뤘던 화학 관련 내용이랑은 느낌이 또 다르네요.&lt;/li&gt;
&lt;li&gt;해당 모델을 개발한 연구원 수만 라부리 또한 영국 기상청의 조언 덕에 문제 해결에 대한 아이디어를 얻을 수 있었던 것 같습니다. 조언이 없었다면 좋지 않은 모델을 개발했을지도 모른다고 언급했다고 합니다(&lt;a href=&#34;https://www.technologyreview.com/2021/09/29/1036331/deepminds-ai-predicts-almost-exactly-when-and-where-its-going-to-rain/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;출처&lt;/a&gt;
).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DGMR paper: &lt;a href=&#34;https://www.nature.com/articles/s41586-021-03854-z&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Skilful precipitation nowcasting using deep generative models of radar&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;MIT Technology Review: &lt;a href=&#34;https://www.technologyreview.com/2021/09/29/1036331/deepminds-ai-predicts-almost-exactly-when-and-where-its-going-to-rain/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;DeepMind’s AI predicts almost exactly when and where it’s going to rain&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Big-O notation</title>
      <link>https://cse-study.github.io/algorithm/2022-03/220322-big-o-notation/</link>
      <pubDate>Tue, 22 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/algorithm/2022-03/220322-big-o-notation/</guid>
      <description>&lt;p&gt;2022년 03월 2주차 개인 목표 수행합니다.&lt;/p&gt;
&lt;p&gt;학계와 산업계에서 말하는 big-$O$ notation에는 약간의 차이가 존재한다는 것을 알게 되었음. 이전부터, 수업에서 배우는 big-$O$와 일반적으로 널리 쓰이는 big-$O$의 의미가 다르다는 점이 헷갈렸는데, 해당 챕터를 읽고 이해할 수 있게 되었음&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(시간복잡도의 경우) 학계에서 big-$O$는 시간의 상한, big-$\Omega$(omega)는 등가 혹은 하한, big-$\Theta$(theta)는 $O$와 $\Omega$ 둘 다를 의미함&lt;/li&gt;
&lt;li&gt;따라서 $O(N)$으로 표현되는 알고리즘을 $O(N^2), O(N^3), O(2^N)$으로도 표현하는 것이, 학계에서는 옳은 표현임&lt;/li&gt;
&lt;li&gt;하지만 산업계에서(면접에서) 말하는 big-$O$의 의미는 학계에서의 $\Theta$의 의미와 가깝다고 생각하면 된다고 함&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;추가적으로, 공간복잡도는 시간이 아닌 메모리(혹은 공간)을 big-$O$로 표현한 것임&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>면접 준비 과정</title>
      <link>https://cse-study.github.io/algorithm/2022-03/220322-coding-interview/</link>
      <pubDate>Tue, 22 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/algorithm/2022-03/220322-coding-interview/</guid>
      <description>&lt;p&gt;Cracking the coding interview 58p까지 읽고 유용했던 내용을 공유합니다. &amp;lt;해외 기업의 채용 절차&amp;gt;와 &amp;lt;지원 전에 준비할 점&amp;gt;에 대한 내용 위주로 느낀점을 공유합니다.&lt;/p&gt;
&lt;p&gt;아무래도 대기업이다보니 중요한 역량은 대부분 일치한다는 것을 알 수 있었는데, 회사마다의 조금씩 다른 특징도 엿볼 수 있었음.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;면접은 보통 전화 통화와 같은 사전 면접 이후에, 3~6번 정도의 대면 면접으로 이루어짐&lt;/li&gt;
&lt;li&gt;마이크로소프트: 괴짜로 불리는 사람들을 원한다(?)고 함. 면접관들이 일하는 곳에서 면접을 본다는 점이 신선했음. 팀 마다 방향이 달라서 어느 팀에 가게 되느냐에 따라 경험이 크게 달라진다고 함&lt;/li&gt;
&lt;li&gt;아마존: 규모 확장성(scalability)에 신경을 많이 쓰며, 객체 지향 디자인에 대해 질문을 많이 하는 경향이 있다고 함.&lt;/li&gt;
&lt;li&gt;구글: 마소, 아마존과 크게 다르지 않으며, 웹 기반으로 시작했기 때문에 구글 또한 규모 확장성에 관심이 많다고 함. 특히 과거 경력에 관계 없이 알고리즘 능력에 큰 비중을 두기 때문에 이 점을 잘 준비해야 함&lt;/li&gt;
&lt;li&gt;페이스북: 개발자 또한 기업가 정신을 갖기를 원하니, 무엇이든 빠르게 결과 내는 사람이라면 좋다고 함&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;면접 전에 갖춰야 할 역량이나 경험에 대해서 다음과 같은 조언도 얻을 수 있었음&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;아직 학생이라면, 큰 규모의 프로젝트 수업을 듣거나 / 인턴 자리를 알아보거나 / 뭔가(프로젝트)를 하라고 함&lt;/li&gt;
&lt;li&gt;이력서는 짧을수록 인상에 남으며, 미국에서는 경력이 십 년 미만인 경우 한 페이지의 이력서를 권장한다고 함&lt;/li&gt;
&lt;li&gt;특정 언어를 사용하는게 낙인이 될 수도 있고, 한 두가지 언어만 알고 있는 경우 구인 담당자가 지원자에 대해 아직 많은 문제를 경험해 보지 못한 사람으로 간주할 수 있다고 함&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;나의 경우엔 현재 모든 작업을 python으로 하고 있는데, 실제 서비스에서는 아무래도 속도가 중요하다보니 C/C++도 가까이 해야겠다는 생각이 들었음&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Self-supervised continual learners</title>
      <link>https://cse-study.github.io/ai/2022-03/220321-self-supervised-coninual-learner/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-03/220321-self-supervised-coninual-learner/</guid>
      <description>&lt;p&gt;최근 1년간 진행된 continual learning 연구 중에서 self-supervised approach를 사용한 논문들에 대해서 간단히 정리합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Co2l: Contrastive continual learning.&amp;rdquo; ICCV 2021&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Asymmetric SupCon loss로 새로운 지식을 학습(novel learning)하고, self-supervised instance-wise relation distill(IRD)로 과거의 지식을 보존(prevent forgetting)&lt;/li&gt;
&lt;li&gt;Asymmetric SupCon: contrastive learning에 current task examples와 memory buffer examples를 둘 다 사용하지만, anchor로는 current task examples만 사용. 이렇게 하면 단순히 contrastive learning 하는 것 보다 효과 좋음&lt;/li&gt;
&lt;li&gt;IRD: reference(previous) model output과 동일해지도록 현재 모델 regulate (2N개 examples에 대해 전부)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;How Well Does Self-Supervised Pre-Training Perform with Streaming ImageNet?.&amp;rdquo; NeruIPS 2021&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Self-supervised learning 방식으로 pre-training하면, streaming data에 대해서 joint training(non-streaming) 만큼의 성능이 나온다는 것을 주장&lt;/li&gt;
&lt;li&gt;Pre-training은 MoCo-v2 protocol을 따르고, OpenSelfSup 라는 prior works의 구현을 기반으로 하였음&lt;/li&gt;
&lt;li&gt;Streaming data의 distribution shift가 조금인 경우에는 joint training과 self-supervised pre-training의 성능이 거의 비슷하고, large distribution shift인 경우에는 MAS(memory aware synapse)와 data replay 방법을 사용하면 비슷해짐&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Rethinking the Representational Continuity: Towards Unsupervised Continual Learning.&amp;rdquo; ICLR 2022&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Label unannotated인 unsupervised continual learning(CURL)을 SimSiam과 Barlow Twining라는 prior works 알고리즘 사용하여 해결해보았더니 신기하게도 supervised continual learning보다 catastrophic forgetting에 강건함&lt;/li&gt;
&lt;li&gt;Lifelong Unsupervised Mixup(LUMP)는 previous task(in memory buffer)와 current task 사이의 interpolate를 활용하는 방법이며, LUMP를 안 써도 잘하지만 LUMP를 사용하면 더 잘함&lt;/li&gt;
&lt;li&gt;Unsupervised continual learning과 supervised continual learning이 low layer에서는 similar하고 high layer에서는 dissimilar함. Unsupervised continual learning의 loss landscape이 더 smooth 함&lt;/li&gt;
&lt;li&gt;Test 단계에서 classifying은 KNN을 사용한다고 하는데, 어떻게 사용한건지 아직 제대로 살펴보진 않았음&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Self-supervised learning이 좋은 representation을 학습한다는 점, 그리고 새로운 지식을 추가적으로 배우는 경우 중에서 특히 해당 데이터의 양이 적을 때 더 효과적이라는 것이 이미 실험적으로 알려져 있었음. 이 장점은 continual learning이 풀고자 하는 문제와 일치하므로 self-supervised learning 방식이 continual learning task에서 효과적일 것이라는 점은 어느정도 예상된 일이었음&lt;/li&gt;
&lt;li&gt;따라서 매우 많은 연구자들이 동시에 해당 문제를 거의 비슷한 방식으로 접근했다는 것이 인상적임&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/html/Cha_Co2L_Contrastive_Continual_Learning_ICCV_2021_paper.html&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Cha, Hyuntak, Jaeho Lee, and Jinwoo Shin. &amp;ldquo;Co2l: Contrastive continual learning.&amp;rdquo; ICCV 2021.&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=gYgMSlZznS&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Hu, Dapeng, et al. &amp;ldquo;How Well Does Self-Supervised Pre-Training Perform with Streaming ImageNet?.&amp;rdquo; NeruIPS 2021.&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=9Hrka5PA7LW&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Madaan, Divyam, et al. &amp;ldquo;Rethinking the Representational Continuity: Towards Unsupervised Continual Learning.&amp;rdquo; ICLR 2022.&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>DopNet</title>
      <link>https://cse-study.github.io/ai/2022-03/20220312-dopnet/</link>
      <pubDate>Sat, 12 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-03/20220312-dopnet/</guid>
      <description>&lt;p&gt;한국화학연구원에서 2021년 7월에 nature computational materials에 발표한 “Predicting thermoelectric properties from chemical formula with explicitly identifying dopant effects”를 읽고 내용을 공유합니다.&lt;/p&gt;
&lt;p&gt;이전 포스팅도 참고 하시면 좋습니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;머신 러닝은 소재 분야에서 많이 쓰이고 있습니다. 만들고자 하는 소재의 물성을 예측하고 어떻게 해야 원하는 특징을 가진 소재를 개발할 수 있을지 도움을 얻을 수 있기 때문입니다.&lt;/li&gt;
&lt;li&gt;하지만 기존 방법들은 도핑된 물질에 대해선 정확한 특성 예측이 힘들어 전문가의 직관에 의존해 개발이 이루어졌습니다. 한국화학연구원은 이 문제를 해결하기 위한 알고리즘을 개발했고 DopNet이라 이름지었습니다.&lt;/li&gt;
&lt;li&gt;기존 알고리즘의 문제는 크게 다음과 같습니다.
&lt;ul&gt;
&lt;li&gt;도핑된 재료의 결정 구조를 찾는데 너무 많은 계산이 필요하다.&lt;/li&gt;
&lt;li&gt;Dopants는 전체에서 낮은 비율을 차지하기 때문에 화학식을 기반으로 한 물질 표현은 해당 효과를 잘 나타내지 못한다(수치적 변화가 크지 않음).&lt;/li&gt;
&lt;li&gt;Dopants가 극적으로 host material의 성질을 바꿀 때 비선형이 크게 나타난다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;이를 해결하기 위해 DopNet은 결정 구조 계산 없이 재료 특성을 예측할 수 있도록 만들었습니다. 그리고 host material과 dopant를 명시적으로 나누어 임베딩하였습니다(기존 방법에서는 dopant 식별 여부와 관계 없이 재료 원자들에 대한 원소 통계 정보만 이용한 것으로 보입니다).&lt;/li&gt;
&lt;li&gt;DopNet은 세 부분으로 구성되어 있습니다.
&lt;ul&gt;
&lt;li&gt;Host material 잠재 임베딩을 추출하는 호스트 임베딩 네트워크&lt;/li&gt;
&lt;li&gt;Dopants 잠재 임베딩을 생성하는 dopants 임베딩 네트워크&lt;/li&gt;
&lt;li&gt;Host material과 dopants의 임베딩을 통해 재료 특성을 예측하는 dense 네트워크&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DopNet이 작동하는 단계는 다음과 같습니다.&lt;/li&gt;
&lt;li&gt;화학식이 입력되면 이는 host material과 dopants로 분해됩니다. 재료의 각 원자는 비율이 γ보다 작거나 같을 때 dopants로 분류되며, γ≥0은 DopNet에 미리 정의되어 있는 hyperparameter입니다.&lt;/li&gt;
&lt;li&gt;Host material는 원소 통계 정보를 기반으로 $\mathbf{x}_h$ 벡터로 표현됩니다. 호스트 특징 벡터에 인코더가 적용되고 호스트 임베딩 네트워크를 통과해 호스트 임베딩이 계산됩니다.&lt;/li&gt;
&lt;li&gt;Dopants의 특징 벡터는 최대 K개의 dopants를 포함할 수 있는 집합 $S_d$에 저장됩니다. 여기서 K는 hyperparameter입니다. Dopants는 dopant 임베딩 네트워크를 통해 임베딩되고 생성된 dopant 임베딩은 단일 벡터 $\mathbf{z}_d$로 표현됩니다.&lt;/li&gt;
&lt;li&gt;마지막으로 dense network를 통해 물성 $\mathbf{y}$ 를 예측합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img  src=&#34;https://cse-study.github.io/assets/img/DopNet_architecture.png&#34;
        alt=&#34;DopNet_architecture&#34;/&gt;&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;이론 분야를 공부하고 있지만 실제 세상의 문제를 해결하기 위해 어떤 노력들과 아이디어도 흥미롭습니다. 특히 학부 전공과 관련된 내용들이다 보니 더 재미있게 읽었습니다.&lt;/li&gt;
&lt;li&gt;내가 하는 연구나 공부가 실제로 어떻게 적용되어 어떤 문제를 해결하고 다른 이들의 연구와 인간의 삶에 어떤 영향을 미칠지 생각하거나 찾아 보는 것도 좋다고 생각합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ngs00/DopNet&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/ngs00/DopNet&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/s41524-021-00564-y&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.nature.com/articles/s41524-021-00564-y&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>POMO: Policy Optimization with Multiple Optima for Reinforcement Learning</title>
      <link>https://cse-study.github.io/ai/2022-03/220306-pomo/</link>
      <pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-03/220306-pomo/</guid>
      <description>&lt;p&gt;Samsung SDS Techtonic의 &amp;ldquo;POMO: 강화학습을 이용한 조합 최적화(NeurIPS 2020)&amp;rdquo; 발표를 보고 느낀점을 공유합니다. 해당 자료는 reference로 걸어두었습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;조합최적화 문제를 강화학습으로 풀려는 시도들이 최근 많이 있음&lt;/li&gt;
&lt;li&gt;강화학습을 처음 공부할 때는 강화학습이 풀 수 있는 실제 세상의 문제가 많이 있을지 의문이었는데, 실제 세상에서 적용할 수 있지만 아직 사람들이 가치를 발견하지 못한 경우가 많겠다는 생각이 들었음&lt;/li&gt;
&lt;li&gt;조합 최적화를 들어보긴 했지만 실제로 어떤 내용인지는 영상을 통해 처음으로 접했음. 내용이 흥미로워서 기초부터 주요 논문까지 쭉 순서대로 제대로 공부해보고 싶다는 생각이 들었음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Dyp9lQpVgCs&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;[Techtonic 2020] Track 1. POMO: 강화학습을 이용한 조합 최적화(NeurIPS 2020) - 권영대 프로&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hAirBRQSFm0&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;[Techtonic 2021] AI를 사용해 기업의 조합최적화 작업을 처리할 수 있을까? (NeurIPS 2021) - 권영대 프로&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/f231f2107df69eab0a3862d50018a9b2-Abstract.html&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;POMO: Policy Optimization with Multiple Optima for Reinforcement Learning&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>