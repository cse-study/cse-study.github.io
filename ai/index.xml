<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ais on CS Study Group</title>
    <link>https://cse-study.github.io/ai/</link>
    <description>Recent content in Ais on CS Study Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>©2019 Notepadium.</copyright>
    <lastBuildDate>Fri, 18 Feb 2022 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://cse-study.github.io/ai/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Data2Vec</title>
      <link>https://cse-study.github.io/ai/2022-02/220218-data2vec/</link>
      <pubDate>Fri, 18 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-02/220218-data2vec/</guid>
      <description>&lt;p&gt;2022년 1월에 Meta AI에서 발표한 &lt;strong&gt;Data2vec&lt;/strong&gt; 블로그 포스팅을 읽고 내용을 공유합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;현재의 self-supervised learning 연구들은 image, speech, text 등 각기 다른 modality마다 학습 방법에 차이가 크다.&lt;/li&gt;
&lt;li&gt;따라서 Meta AI는 multiple modality(e.b. image, speech, text)에서 작동하는 data2vec을 개발하였으며, data2vec은 기존 computer vision, speech 분야의 알고리즘 성능을 넘었고, NLP 분야에서는 견줄만큼의 성능을 기록하였다.&lt;/li&gt;
&lt;li&gt;Data2vec은 기존에 self-supervised learning에서 자주 사용되던 contrastive learning이나, reconstructing the input example 방식을 사용하지는 않는다.&lt;/li&gt;
&lt;li&gt;Data2vec은 input data에 대한 own representation을 맞추도록 학습시킨다.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Teacher network에 데이터를 넣어 출력된 representation을 target으로 설정한다.&lt;/li&gt;
&lt;li&gt;Student network에는 데이터에 일부 masking을 가해서 넣어 representation을 얻어내고, 이 representation이 teacher의 target representation과 동일해지도록 모델을 학습시킨다.&lt;/li&gt;
&lt;li&gt;Teacher와 student는 동일한 네트워크이지만 weight 값이 살짝 다르다. (&lt;a href=&#34;https://yuhodots.github.io/deeplearning/21-04-04/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;BYOL&lt;/a&gt;
 처럼 exponentialmoving average 사용)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Self-supervised learning 중에서도 BYOL의 아이디어를 차용하여 multi-modal learning 알고리즘을 고안한 것이 재미있었음&lt;/li&gt;
&lt;li&gt;하나의 backbone으로 image, speech, text 관련 task에 모두 적용 가능하다는 것이, 현재까지의 multi-modal 연구 중에 가장 자연스러운 방법이라는 생각이 들며 future work들이 기대됨&lt;/li&gt;
&lt;li&gt;Label이 필요없고, multi-modality에서 작동하고, single-purpose algorithms보다 잘하는 AI 모델. 이것이 앞으로의 AI의 주요 방향성이 되지 않을까 싶음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data2vec블로그 포스팅: &lt;a href=&#34;https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;The first high-performance self-supervised algorithm that works for speech, vision, and text&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Data2vec paper: &lt;a href=&#34;https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Data2vec code: &lt;a href=&#34;https://github.com/pytorch/fairseq/tree/main/examples/data2vec&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/pytorch/fairseq/tree/main/examples/data2vec&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>GT Sophy</title>
      <link>https://cse-study.github.io/ai/2022-02/20220214-gt-sophy/</link>
      <pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-02/20220214-gt-sophy/</guid>
      <description>&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;img src=&#34;https://media.springernature.com/w200/springer-static/cover-hires/journal/41586/602/7896&#34; alt=&#34;Volume 602 Issue 7896&#34; style=&#34;zoom:100%; display: block; margin-left: auto; margin-right: auto;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;2022년 2월 소니 AI가 개발한 자동차 경주 AI가 네이처 표지를 장식했음. 게임 이름은 그란 투스리모 스포츠임.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;인공지능 이름은 GT Sophy. Model free, off-policy deepRL을 이용했음. Distributional SAC과 비슷하지만 value backup 과 target functions가 다른 QR-SAC을 이용했다고 함.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;게임의 리얼리즘 때문에 생각보다 어려웠다고 함. 또한 규정화되어 있지 않은 스포츠맨십을 지키며 너무 공격적이지도 소극적이지도 않게 플레이 하기 위한 보상 체계를 만드는 것이 힘들었다고 함.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;학습 방법: PS4에서 실행되는 GT인스턴스를 제어하는 rollout worker에게 트레이닝 시나리오를 배포하면 각 agent는 가장 최근 policy의 사본을 실행시키고 action을 전송함. 비동기적으로 다음 프레임을 계산해 새로운 상태를 알아냄. State, action, reward 튜플을 ERB에 저장함. 트레이너는 policy를 업데이트하기 위해 ERB를 샘플링함.&lt;/p&gt;
&lt;img src=&#34;https://cse-study.github.io/assets/img/image-20220214181447986.png&#34; alt=&#34;image-20220214181447986&#34; style=&#34;zoom:30%; display: block; margin-left: auto; margin-right: auto;&#34; /&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;강화학습을 이용해 게임에 적용시키는 경우가 점점 늘어나는 것 같음. 적용시키기 좋고, 흥미로우며 이슈도 되기 때문이라 생각함. 개인적으로도 해 보고 싶다는 생각이 들었음.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AI vs 사람 구도의 실험에서는 AI와 사람 간 공평성을 위한 적절한 방법들이 고려되어야 함. 예를 들어 알파스타의 경우 평균 APM에 제한을 두었지만 중요한 교전 상황에 압도적인 순간 APM을 보이며 승리했음. 사람의 경우 APM이 모두 의미 있는 동작을 뜻하지 않으므로 실제 차이는 더 컸었다 해석할 수 있음. 해당 실험에서도 여러 사항을 고려하여 제약을 두었음. 논문의 Fairness versus humans에서 찾아볼 수 있음.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;개발진들이 이런 연구를 하는 이유는 사람과 싸워 이기는 AI를 만들기 위함이 아니라 사람을 더 잘 이해하고, 더 재미있으며 흥미로운 탐험과 새로운 경험을 사용자에게 제공하기 위함이라 함.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;연구의 의미를 항상 생각하는 것이 중요함을 다시 한 번 느꼈음. 새로운 방법과 좋은 성능 자체도 의미있지만 이들이 앞으로의 연구와 우리의 삶에 어떤 긍정적인 역할을 할지 깊게 생각한다면 더 좋을 것 같다는 생각을 했음.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/s41586-021-04357-7&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.nature.com/articles/s41586-021-04357-7&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/l948hMaTPuo&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://youtu.be/l948hMaTPuo&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>AlphaCode</title>
      <link>https://cse-study.github.io/ai/2022-02/220205-alphacode/</link>
      <pubDate>Sat, 05 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/ai/2022-02/220205-alphacode/</guid>
      <description>&lt;p&gt;2022년 2월에 딥마인드에서 발표한 &lt;strong&gt;AlphaCode&lt;/strong&gt; 블로그 포스팅을 읽고 내용을 공유합니다.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;AlphaCode란?: competitive programming task를 human-level로 풀 수 있는 시스템. Codeforces의 10개 대회에 대해 테스트 했을 때, 참가자들의 평균 수준의 성능 달성 (상위 54%)&lt;/li&gt;
&lt;li&gt;Model: Transformer-based langauge model 이라고 포스팅에 언급되어 있음. Language generation(code generation)이 주요 역할인 것으로 보아 decoder-centric langauge model 중 하나를 사용했을 것으로 생각됨. 그 뒤에 promising program으로 결과를 필터링했다고 하는데, rule-based로 결과를 필터링 했다는 의미로 대충 이해했음 (promising program의 정확한 의미를 아는 분 있으면 코멘트 부탁드립니다)&lt;/li&gt;
&lt;li&gt;Training: Github code로 pre-training하고, competitive programming task에 대해 fine-tuning 진행하였음 (일반적인 language model 학습하는 방식과 동일하게)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Attention 기반 시각화가 너무 아름다움🥺 기회가 되면 나중에 한번 만들어보고 싶음&lt;/li&gt;
&lt;li&gt;포스팅 원문에서 방법론 추가에 따른 모델의 발전과정을 볼 수 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Alphacode 블로그 포스팅: &lt;a href=&#34;https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Competitive programming with AlphaCode&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Alphacode 시각화: &lt;a href=&#34;https://alphacode.deepmind.com/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;AlphaCode Attention Visualization&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Alphacode paper: &lt;a href=&#34;https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Alphacode dataset: &lt;a href=&#34;https://github.com/deepmind/code_contests&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/deepmind/code_contests&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>https://cse-study.github.io/help/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/help/</guid>
      <description>&lt;h3 id=&#34;shortcuts&#34;&gt;Shortcuts&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;스터디에 도움이되는 자료들을 모아두는 공간입니다. 좋은 자료가 있다면 아래에 계속 추가해주세요 🙌&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://papers.labml.ai/papers/daily/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;DAILY PAPERS&lt;/a&gt;
: 최신 인기있는 AI 논문을 찾아주는 웹 사이트입니다.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/YannicKilcher/videos&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;YannicKilcher youtube&lt;/a&gt;
: 최신 AI 기술을 요약하여 설명해주는 유튜브 채널입니다.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/MachineLearning/&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Reddit, ML 카테고리&lt;/a&gt;
: AI 관련 소식을 공유하는 커뮤니티입니다.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://paperswithcode.com/newsletter&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Papers with Code, 뉴스레터&lt;/a&gt;
: 최신 인기있는 AI 자료들을 공유해주는 뉴스레터입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-post&#34;&gt;How to post&lt;/h3&gt;
&lt;p&gt;&lt;img  src=&#34;https://cse-study.github.io/assets/img/notice1.png&#34;
        alt=&#34;img&#34;/&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;+++
&lt;span class=&#34;nv&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;제목을 입력하세요.&amp;#34;&lt;/span&gt;
&lt;span class=&#34;nv&#34;&gt;date&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;2022-01-01&amp;#34;&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;profile&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
	&lt;span class=&#34;nb&#34;&gt;enable&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt;
	&lt;span class=&#34;nv&#34;&gt;avatar&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;/assets/img/profile.jpg&amp;#34;&lt;/span&gt;
	&lt;span class=&#34;nv&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;홍길동&amp;#34;&lt;/span&gt;
	&lt;span class=&#34;nv&#34;&gt;email&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;hong-gildong@gmail.com&amp;#34;&lt;/span&gt;
+++

포스팅의 미리보기 입니다.
&amp;lt;!--more--&amp;gt;

&amp;lt;mark&amp;gt;본문입니다.&amp;lt;/mark&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;마크다운 파일의 제목은 &lt;code&gt;작성날짜-제목.md&lt;/code&gt;의 형태로 설정해주시면 좋습니다.&lt;/li&gt;
&lt;li&gt;각 스터디의 github repository에 마크다운 파일을 push하여 블로그에 포스팅을 업로드 할 수 있습니다.&lt;/li&gt;
&lt;li&gt;💡 &lt;code&gt;Notice&lt;/code&gt;: 자동화가 아직 적용되지 않았습니다. 배포까지의 자세한 과정은 관리자에게 문의해주세요!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;image-upload&#34;&gt;Image upload&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;프로필 이미지 업로드 방법은 다음과 같습니다. (아직 작성하지 않았습니다.)&lt;/li&gt;
&lt;li&gt;게시글 이미지 업로드 방법은 다음과 같습니다. (아직 작성하지 않았습니다.)&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Hello 👋</title>
      <link>https://cse-study.github.io/home/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cse-study.github.io/home/</guid>
      <description>&lt;p&gt;안녕하세요, 방문해주셔서 감사합니다. 저희는 이 곳에서 CS 관련 스터디를 진행하고 있습니다. 현재는 알고리즘/코테 스터디와, 간단한 AI 스터디를 진행하고 있습니다!&lt;/p&gt;
&lt;h3 id=&#34;weekly-ai&#34;&gt;Weekly AI&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;AI 관련 소식, 기술 블로그 글, 유튜브 영상 등 관심있는 자료를 보고 &lt;mark&gt;간단히 요약&lt;/mark&gt;합니다. 느낀점, 한줄평을 추가해주셔도 좋습니다.&lt;/li&gt;
&lt;li&gt;게시글은 &lt;mark&gt;매주 1인당 1개씩 작성&lt;/mark&gt;합니다. 최신 AI 기술을 파악하는 것이 목적이므로 2년 이내의 자료만 허용합니다.&lt;/li&gt;
&lt;li&gt;어떤 자료를 참고하였는지 다른 사람들도 확인할 수 있도록 &lt;mark&gt;참고한 자료의 링크를 reference&lt;/mark&gt; 달아주시면 좋습니다.&lt;/li&gt;
&lt;li&gt;유명하거나, 임팩트있는 기술이 아니어도 상관 없으며, 각자 관심있는 분야의 AI 소식을 매 주 받아들이면서 서로의 시야가 넓어지는 것이 스터디의 주요 목표입니다.&lt;/li&gt;
&lt;li&gt;서로의 게시글에서 궁금한 점이 있다면 적극적으로 코멘트 해주세요!&lt;/li&gt;
&lt;li&gt;항상 저작권 문제를 고려하며 게시글을 작성해주시면 감사하겠습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;아직 작성하지 않았습니다.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>